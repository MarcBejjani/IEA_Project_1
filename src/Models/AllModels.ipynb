{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8ApUsIg6Uhv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "from sklearn.preprocessing import scale\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIrjtmvQ6jXs"
      },
      "outputs": [],
      "source": [
        "#import file and reading few lines\n",
        "# dataTrain = pd.read_csv('FeaturesWhite.csv')\n",
        "# dataEval = pd.read_csv('EvalWithWhite.csv')\n",
        "# dataTrain = pd.read_csv('FeaturesNoWhite.csv')\n",
        "# dataEval = pd.read_csv('EvalWithNoWhite.csv')\n",
        "# dataTrain = pd.read_csv('HistogramWhite.csv')\n",
        "# dataEval = pd.read_csv('EvalHistogramWhite.csv')\n",
        "dataTrain = pd.read_csv('ProfileWhite.csv')\n",
        "dataEval = pd.read_csv('EvalProfileWhite.csv')\n",
        "#dataTrain.head(10)\n",
        "file = open('Accuracies.txt', 'a')\n",
        "listOfLines=[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtsfLm7L7M_O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccb8639f-f9b7-48e6-87b1-0746aa372473"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2728, 824)\n",
            "(682, 824)\n"
          ]
        }
      ],
      "source": [
        "print(dataTrain.shape)\n",
        "print(dataEval.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnames = list(dataTrain.columns)\n",
        "print(cnames)"
      ],
      "metadata": {
        "id": "7pctAEZam-p4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff949727-dc1d-4cf9-b46c-0ab06c5151d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['image', 'label', 'BlackToWhite', 'Aspect Ratio', 'Column Histogram 0', 'Column Histogram 1', 'Column Histogram 2', 'Column Histogram 3', 'Column Histogram 4', 'Column Histogram 5', 'Column Histogram 6', 'Column Histogram 7', 'Column Histogram 8', 'Column Histogram 9', 'Column Histogram 10', 'Column Histogram 11', 'Column Histogram 12', 'Column Histogram 13', 'Column Histogram 14', 'Column Histogram 15', 'Column Histogram 16', 'Column Histogram 17', 'Column Histogram 18', 'Column Histogram 19', 'Column Histogram 20', 'Column Histogram 21', 'Column Histogram 22', 'Column Histogram 23', 'Column Histogram 24', 'Column Histogram 25', 'Column Histogram 26', 'Column Histogram 27', 'Column Histogram 28', 'Column Histogram 29', 'Row Histogram 0', 'Row Histogram 1', 'Row Histogram 2', 'Row Histogram 3', 'Row Histogram 4', 'Row Histogram 5', 'Row Histogram 6', 'Row Histogram 7', 'Row Histogram 8', 'Row Histogram 9', 'Row Histogram 10', 'Row Histogram 11', 'Row Histogram 12', 'Row Histogram 13', 'Row Histogram 14', 'Row Histogram 15', 'Row Histogram 16', 'Row Histogram 17', 'Row Histogram 18', 'Row Histogram 19', 'Row Histogram 20', 'Row Histogram 21', 'Row Histogram 22', 'Row Histogram 23', 'Row Histogram 24', 'Row Histogram 25', 'Row Histogram 26', 'Row Histogram 27', 'Row Histogram 28', 'Row Histogram 29', 'Bottom Profile 0', 'Bottom Profile 1', 'Bottom Profile 2', 'Bottom Profile 3', 'Bottom Profile 4', 'Bottom Profile 5', 'Bottom Profile 6', 'Bottom Profile 7', 'Bottom Profile 8', 'Bottom Profile 9', 'Bottom Profile 10', 'Bottom Profile 11', 'Bottom Profile 12', 'Bottom Profile 13', 'Bottom Profile 14', 'Bottom Profile 15', 'Bottom Profile 16', 'Bottom Profile 17', 'Bottom Profile 18', 'Bottom Profile 19', 'Bottom Profile 20', 'Bottom Profile 21', 'Bottom Profile 22', 'Bottom Profile 23', 'Bottom Profile 24', 'Bottom Profile 25', 'Bottom Profile 26', 'Bottom Profile 27', 'Bottom Profile 28', 'Bottom Profile 29', 'Top Profile 0', 'Top Profile 1', 'Top Profile 2', 'Top Profile 3', 'Top Profile 4', 'Top Profile 5', 'Top Profile 6', 'Top Profile 7', 'Top Profile 8', 'Top Profile 9', 'Top Profile 10', 'Top Profile 11', 'Top Profile 12', 'Top Profile 13', 'Top Profile 14', 'Top Profile 15', 'Top Profile 16', 'Top Profile 17', 'Top Profile 18', 'Top Profile 19', 'Top Profile 20', 'Top Profile 21', 'Top Profile 22', 'Top Profile 23', 'Top Profile 24', 'Top Profile 25', 'Top Profile 26', 'Top Profile 27', 'Top Profile 28', 'Top Profile 29', 'Left Profile 0', 'Left Profile 1', 'Left Profile 2', 'Left Profile 3', 'Left Profile 4', 'Left Profile 5', 'Left Profile 6', 'Left Profile 7', 'Left Profile 8', 'Left Profile 9', 'Left Profile 10', 'Left Profile 11', 'Left Profile 12', 'Left Profile 13', 'Left Profile 14', 'Left Profile 15', 'Left Profile 16', 'Left Profile 17', 'Left Profile 18', 'Left Profile 19', 'Left Profile 20', 'Left Profile 21', 'Left Profile 22', 'Left Profile 23', 'Left Profile 24', 'Left Profile 25', 'Left Profile 26', 'Left Profile 27', 'Left Profile 28', 'Left Profile 29', 'Right Profile 0', 'Right Profile 1', 'Right Profile 2', 'Right Profile 3', 'Right Profile 4', 'Right Profile 5', 'Right Profile 6', 'Right Profile 7', 'Right Profile 8', 'Right Profile 9', 'Right Profile 10', 'Right Profile 11', 'Right Profile 12', 'Right Profile 13', 'Right Profile 14', 'Right Profile 15', 'Right Profile 16', 'Right Profile 17', 'Right Profile 18', 'Right Profile 19', 'Right Profile 20', 'Right Profile 21', 'Right Profile 22', 'Right Profile 23', 'Right Profile 24', 'Right Profile 25', 'Right Profile 26', 'Right Profile 27', 'Right Profile 28', 'Right Profile 29', 'HOG 0', 'HOG 1', 'HOG 2', 'HOG 3', 'HOG 4', 'HOG 5', 'HOG 6', 'HOG 7', 'HOG 8', 'HOG 9', 'HOG 10', 'HOG 11', 'HOG 12', 'HOG 13', 'HOG 14', 'HOG 15', 'HOG 16', 'HOG 17', 'HOG 18', 'HOG 19', 'HOG 20', 'HOG 21', 'HOG 22', 'HOG 23', 'HOG 24', 'HOG 25', 'HOG 26', 'HOG 27', 'HOG 28', 'HOG 29', 'HOG 30', 'HOG 31', 'HOG 32', 'HOG 33', 'HOG 34', 'HOG 35', 'HOG 36', 'HOG 37', 'HOG 38', 'HOG 39', 'HOG 40', 'HOG 41', 'HOG 42', 'HOG 43', 'HOG 44', 'HOG 45', 'HOG 46', 'HOG 47', 'HOG 48', 'HOG 49', 'HOG 50', 'HOG 51', 'HOG 52', 'HOG 53', 'HOG 54', 'HOG 55', 'HOG 56', 'HOG 57', 'HOG 58', 'HOG 59', 'HOG 60', 'HOG 61', 'HOG 62', 'HOG 63', 'HOG 64', 'HOG 65', 'HOG 66', 'HOG 67', 'HOG 68', 'HOG 69', 'HOG 70', 'HOG 71', 'HOG 72', 'HOG 73', 'HOG 74', 'HOG 75', 'HOG 76', 'HOG 77', 'HOG 78', 'HOG 79', 'HOG 80', 'HOG 81', 'HOG 82', 'HOG 83', 'HOG 84', 'HOG 85', 'HOG 86', 'HOG 87', 'HOG 88', 'HOG 89', 'HOG 90', 'HOG 91', 'HOG 92', 'HOG 93', 'HOG 94', 'HOG 95', 'HOG 96', 'HOG 97', 'HOG 98', 'HOG 99', 'HOG 100', 'HOG 101', 'HOG 102', 'HOG 103', 'HOG 104', 'HOG 105', 'HOG 106', 'HOG 107', 'HOG 108', 'HOG 109', 'HOG 110', 'HOG 111', 'HOG 112', 'HOG 113', 'HOG 114', 'HOG 115', 'HOG 116', 'HOG 117', 'HOG 118', 'HOG 119', 'HOG 120', 'HOG 121', 'HOG 122', 'HOG 123', 'HOG 124', 'HOG 125', 'HOG 126', 'HOG 127', 'HOG 128', 'HOG 129', 'HOG 130', 'HOG 131', 'HOG 132', 'HOG 133', 'HOG 134', 'HOG 135', 'HOG 136', 'HOG 137', 'HOG 138', 'HOG 139', 'HOG 140', 'HOG 141', 'HOG 142', 'HOG 143', 'HOG 144', 'HOG 145', 'HOG 146', 'HOG 147', 'HOG 148', 'HOG 149', 'HOG 150', 'HOG 151', 'HOG 152', 'HOG 153', 'HOG 154', 'HOG 155', 'HOG 156', 'HOG 157', 'HOG 158', 'HOG 159', 'HOG 160', 'HOG 161', 'HOG 162', 'HOG 163', 'HOG 164', 'HOG 165', 'HOG 166', 'HOG 167', 'HOG 168', 'HOG 169', 'HOG 170', 'HOG 171', 'HOG 172', 'HOG 173', 'HOG 174', 'HOG 175', 'HOG 176', 'HOG 177', 'HOG 178', 'HOG 179', 'HOG 180', 'HOG 181', 'HOG 182', 'HOG 183', 'HOG 184', 'HOG 185', 'HOG 186', 'HOG 187', 'HOG 188', 'HOG 189', 'HOG 190', 'HOG 191', 'HOG 192', 'HOG 193', 'HOG 194', 'HOG 195', 'HOG 196', 'HOG 197', 'HOG 198', 'HOG 199', 'HOG 200', 'HOG 201', 'HOG 202', 'HOG 203', 'HOG 204', 'HOG 205', 'HOG 206', 'HOG 207', 'HOG 208', 'HOG 209', 'HOG 210', 'HOG 211', 'HOG 212', 'HOG 213', 'HOG 214', 'HOG 215', 'HOG 216', 'HOG 217', 'HOG 218', 'HOG 219', 'HOG 220', 'HOG 221', 'HOG 222', 'HOG 223', 'HOG 224', 'HOG 225', 'HOG 226', 'HOG 227', 'HOG 228', 'HOG 229', 'HOG 230', 'HOG 231', 'HOG 232', 'HOG 233', 'HOG 234', 'HOG 235', 'HOG 236', 'HOG 237', 'HOG 238', 'HOG 239', 'HOG 240', 'HOG 241', 'HOG 242', 'HOG 243', 'HOG 244', 'HOG 245', 'HOG 246', 'HOG 247', 'HOG 248', 'HOG 249', 'HOG 250', 'HOG 251', 'HOG 252', 'HOG 253', 'HOG 254', 'HOG 255', 'HOG 256', 'HOG 257', 'HOG 258', 'HOG 259', 'HOG 260', 'HOG 261', 'HOG 262', 'HOG 263', 'HOG 264', 'HOG 265', 'HOG 266', 'HOG 267', 'HOG 268', 'HOG 269', 'HOG 270', 'HOG 271', 'HOG 272', 'HOG 273', 'HOG 274', 'HOG 275', 'HOG 276', 'HOG 277', 'HOG 278', 'HOG 279', 'HOG 280', 'HOG 281', 'HOG 282', 'HOG 283', 'HOG 284', 'HOG 285', 'HOG 286', 'HOG 287', 'HOG 288', 'HOG 289', 'HOG 290', 'HOG 291', 'HOG 292', 'HOG 293', 'HOG 294', 'HOG 295', 'HOG 296', 'HOG 297', 'HOG 298', 'HOG 299', 'HOG 300', 'HOG 301', 'HOG 302', 'HOG 303', 'HOG 304', 'HOG 305', 'HOG 306', 'HOG 307', 'HOG 308', 'HOG 309', 'HOG 310', 'HOG 311', 'HOG 312', 'HOG 313', 'HOG 314', 'HOG 315', 'HOG 316', 'HOG 317', 'HOG 318', 'HOG 319', 'HOG 320', 'HOG 321', 'HOG 322', 'HOG 323', 'HOG 324', 'HOG 325', 'HOG 326', 'HOG 327', 'HOG 328', 'HOG 329', 'HOG 330', 'HOG 331', 'HOG 332', 'HOG 333', 'HOG 334', 'HOG 335', 'HOG 336', 'HOG 337', 'HOG 338', 'HOG 339', 'HOG 340', 'HOG 341', 'HOG 342', 'HOG 343', 'HOG 344', 'HOG 345', 'HOG 346', 'HOG 347', 'HOG 348', 'HOG 349', 'HOG 350', 'HOG 351', 'HOG 352', 'HOG 353', 'HOG 354', 'HOG 355', 'HOG 356', 'HOG 357', 'HOG 358', 'HOG 359', 'HOG 360', 'HOG 361', 'HOG 362', 'HOG 363', 'HOG 364', 'HOG 365', 'HOG 366', 'HOG 367', 'HOG 368', 'HOG 369', 'HOG 370', 'HOG 371', 'HOG 372', 'HOG 373', 'HOG 374', 'HOG 375', 'HOG 376', 'HOG 377', 'HOG 378', 'HOG 379', 'HOG 380', 'HOG 381', 'HOG 382', 'HOG 383', 'HOG 384', 'HOG 385', 'HOG 386', 'HOG 387', 'HOG 388', 'HOG 389', 'HOG 390', 'HOG 391', 'HOG 392', 'HOG 393', 'HOG 394', 'HOG 395', 'HOG 396', 'HOG 397', 'HOG 398', 'HOG 399', 'HOG 400', 'HOG 401', 'HOG 402', 'HOG 403', 'HOG 404', 'HOG 405', 'HOG 406', 'HOG 407', 'HOG 408', 'HOG 409', 'HOG 410', 'HOG 411', 'HOG 412', 'HOG 413', 'HOG 414', 'HOG 415', 'HOG 416', 'HOG 417', 'HOG 418', 'HOG 419', 'HOG 420', 'HOG 421', 'HOG 422', 'HOG 423', 'HOG 424', 'HOG 425', 'HOG 426', 'HOG 427', 'HOG 428', 'HOG 429', 'HOG 430', 'HOG 431', 'HOG 432', 'HOG 433', 'HOG 434', 'HOG 435', 'HOG 436', 'HOG 437', 'HOG 438', 'HOG 439', 'HOG 440', 'HOG 441', 'HOG 442', 'HOG 443', 'HOG 444', 'HOG 445', 'HOG 446', 'HOG 447', 'HOG 448', 'HOG 449', 'HOG 450', 'HOG 451', 'HOG 452', 'HOG 453', 'HOG 454', 'HOG 455', 'HOG 456', 'HOG 457', 'HOG 458', 'HOG 459', 'HOG 460', 'HOG 461', 'HOG 462', 'HOG 463', 'HOG 464', 'HOG 465', 'HOG 466', 'HOG 467', 'HOG 468', 'HOG 469', 'HOG 470', 'HOG 471', 'HOG 472', 'HOG 473', 'HOG 474', 'HOG 475', 'HOG 476', 'HOG 477', 'HOG 478', 'HOG 479', 'HOG 480', 'HOG 481', 'HOG 482', 'HOG 483', 'HOG 484', 'HOG 485', 'HOG 486', 'HOG 487', 'HOG 488', 'HOG 489', 'HOG 490', 'HOG 491', 'HOG 492', 'HOG 493', 'HOG 494', 'HOG 495', 'HOG 496', 'HOG 497', 'HOG 498', 'HOG 499', 'HOG 500', 'HOG 501', 'HOG 502', 'HOG 503', 'HOG 504', 'HOG 505', 'HOG 506', 'HOG 507', 'HOG 508', 'HOG 509', 'HOG 510', 'HOG 511', 'HOG 512', 'HOG 513', 'HOG 514', 'HOG 515', 'HOG 516', 'HOG 517', 'HOG 518', 'HOG 519', 'HOG 520', 'HOG 521', 'HOG 522', 'HOG 523', 'HOG 524', 'HOG 525', 'HOG 526', 'HOG 527', 'HOG 528', 'HOG 529', 'HOG 530', 'HOG 531', 'HOG 532', 'HOG 533', 'HOG 534', 'HOG 535', 'HOG 536', 'HOG 537', 'HOG 538', 'HOG 539', 'HOG 540', 'HOG 541', 'HOG 542', 'HOG 543', 'HOG 544', 'HOG 545', 'HOG 546', 'HOG 547', 'HOG 548', 'HOG 549', 'HOG 550', 'HOG 551', 'HOG 552', 'HOG 553', 'HOG 554', 'HOG 555', 'HOG 556', 'HOG 557', 'HOG 558', 'HOG 559', 'HOG 560', 'HOG 561', 'HOG 562', 'HOG 563', 'HOG 564', 'HOG 565', 'HOG 566', 'HOG 567', 'HOG 568', 'HOG 569', 'HOG 570', 'HOG 571', 'HOG 572', 'HOG 573', 'HOG 574', 'HOG 575', 'HOG 576', 'HOG 577', 'HOG 578', 'HOG 579', 'HOG 580', 'HOG 581', 'HOG 582', 'HOG 583', 'HOG 584', 'HOG 585', 'HOG 586', 'HOG 587', 'HOG 588', 'HOG 589', 'HOG 590', 'HOG 591', 'HOG 592', 'HOG 593', 'HOG 594', 'HOG 595', 'HOG 596', 'HOG 597', 'HOG 598', 'HOG 599', 'HOG 600', 'HOG 601', 'HOG 602', 'HOG 603', 'HOG 604', 'HOG 605', 'HOG 606', 'HOG 607', 'HOG 608', 'HOG 609', 'HOG 610', 'HOG 611', 'HOG 612', 'HOG 613', 'HOG 614', 'HOG 615', 'HOG 616', 'HOG 617', 'HOG 618', 'HOG 619', 'HOG 620', 'HOG 621', 'HOG 622', 'HOG 623', 'HOG 624', 'HOG 625', 'HOG 626', 'HOG 627', 'HOG 628', 'HOG 629', 'HOG 630', 'HOG 631', 'HOG 632', 'HOG 633', 'HOG 634', 'HOG 635', 'HOG 636', 'HOG 637', 'HOG 638', 'HOG 639']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ndataTrain = dataTrain\n",
        "ndataEval = dataEval\n",
        "\n",
        "cnames = list(dataTrain.columns)\n",
        "\n",
        "#remove HOG\n",
        "# for c in cnames:\n",
        "#   if c.find('HOG') != -1:\n",
        "#     ndataTrain = ndataTrain.drop(c,axis =1)\n",
        "#     ndataEval = ndataEval.drop(c,axis =1)\n",
        "\n",
        "\n",
        "#remove Profile\n",
        "# for c in cnames:\n",
        "#   if c.find('Top') != -1 or c.find('Right') != -1 or c.find('Left') != -1 or c.find('Top') != -1 or c.find('Bottom') != -1:\n",
        "#     ndataTrain = ndataTrain.drop(c,axis =1)\n",
        "#     ndataEval = ndataEval.drop(c,axis =1)\n",
        "\n",
        "\n",
        "# remove Histogram\n",
        "# for c in cnames:\n",
        "#   if c.find('Histogram') != -1:\n",
        "#     ndataTrain = ndataTrain.drop(c,axis =1)\n",
        "#     ndataEval = ndataEval.drop(c,axis =1)\n",
        "\n",
        "\n",
        "# remove Symmetry\n",
        "# for c in cnames:\n",
        "#   if c.find('Symmetry') != -1:\n",
        "#     ndataTrain = ndataTrain.drop(c,axis =1)\n",
        "#     ndataEval = ndataEval.drop(c,axis =1)\n"
      ],
      "metadata": {
        "id": "V91HDK1pBc0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJh0mGU89EcY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "outputId": "aec637fb-d27b-4f47-d7cd-f67e1bb8610a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       BlackToWhite  Aspect Ratio  Column Histogram 0  Column Histogram 1  \\\n",
              "count   2728.000000   2728.000000         2728.000000         2728.000000   \n",
              "mean       0.920930      0.793757            0.032806            0.075754   \n",
              "std        0.574493      0.256534            0.079144            0.150864   \n",
              "min        0.250764      0.145482            0.000000            0.000000   \n",
              "5%         0.416202      0.444991            0.000000            0.000000   \n",
              "10%        0.474304      0.525757            0.000000            0.000000   \n",
              "25%        0.613739      0.636770            0.000000            0.000000   \n",
              "50%        0.789889      0.768109            0.000000            0.000000   \n",
              "75%        1.073029      0.919418            0.000000            0.100000   \n",
              "90%        1.423110      1.106355            0.133333            0.300000   \n",
              "99%        2.853005      1.611474            0.353179            0.664448   \n",
              "max       10.400259      2.137500            0.831373            0.966667   \n",
              "\n",
              "       Column Histogram 2  Column Histogram 3  Column Histogram 4  \\\n",
              "count         2728.000000         2728.000000         2728.000000   \n",
              "mean             0.123485            0.187879            0.262204   \n",
              "std              0.201066            0.245763            0.282415   \n",
              "min              0.000000            0.000000            0.000000   \n",
              "5%               0.000000            0.000000            0.000000   \n",
              "10%              0.000000            0.000000            0.000000   \n",
              "25%              0.000000            0.000000            0.000000   \n",
              "50%              0.000000            0.039412            0.166667   \n",
              "75%              0.200000            0.333333            0.466667   \n",
              "90%              0.455556            0.566667            0.700000   \n",
              "99%              0.800000            0.900000            0.966667   \n",
              "max              1.000000            1.000000            1.000000   \n",
              "\n",
              "       Column Histogram 5  Column Histogram 6  Column Histogram 7  ...  \\\n",
              "count         2728.000000         2728.000000         2728.000000  ...   \n",
              "mean             0.337069            0.405489            0.451683  ...   \n",
              "std              0.295835            0.295638            0.281953  ...   \n",
              "min              0.000000            0.000000            0.000000  ...   \n",
              "5%               0.000000            0.000000            0.000000  ...   \n",
              "10%              0.000000            0.000000            0.100000  ...   \n",
              "25%              0.050654            0.166667            0.233333  ...   \n",
              "50%              0.279020            0.366667            0.421111  ...   \n",
              "75%              0.566667            0.633333            0.666667  ...   \n",
              "90%              0.766667            0.860235            0.895961  ...   \n",
              "99%              1.000000            1.000000            1.000000  ...   \n",
              "max              1.000000            1.000000            1.000000  ...   \n",
              "\n",
              "           HOG 630      HOG 631      HOG 632      HOG 633      HOG 634  \\\n",
              "count  2728.000000  2728.000000  2728.000000  2728.000000  2728.000000   \n",
              "mean      0.028547     0.002413     0.023749     0.009320     0.118547   \n",
              "std       0.065096     0.010147     0.056216     0.024494     0.103208   \n",
              "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "5%        0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "10%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "50%       0.000000     0.000000     0.000000     0.000000     0.117437   \n",
              "75%       0.000000     0.000000     0.011913     0.000000     0.218082   \n",
              "90%       0.126002     0.000000     0.078463     0.033085     0.257358   \n",
              "99%       0.262406     0.054255     0.258622     0.123749     0.297355   \n",
              "max       0.296387     0.129691     0.328205     0.259843     0.371676   \n",
              "\n",
              "           HOG 635      HOG 636      HOG 637      HOG 638      HOG 639  \n",
              "count  2728.000000  2728.000000  2728.000000  2728.000000  2728.000000  \n",
              "mean      0.027727     0.116288     0.015298     0.028397     0.002088  \n",
              "std       0.033364     0.122328     0.035007     0.055567     0.008343  \n",
              "min       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
              "5%        0.000000     0.000000     0.000000     0.000000     0.000000  \n",
              "10%       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
              "25%       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
              "50%       0.016410     0.076482     0.000000     0.000000     0.000000  \n",
              "75%       0.045562     0.239293     0.000000     0.036160     0.000000  \n",
              "90%       0.072007     0.284325     0.064247     0.103458     0.000000  \n",
              "99%       0.134428     0.440280     0.155178     0.245899     0.043373  \n",
              "max       0.252678     0.495138     0.312616     0.312497     0.079679  \n",
              "\n",
              "[12 rows x 822 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-82f0079e-9cd0-4899-87a9-61bc2d552c8b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>BlackToWhite</th>\n",
              "      <th>Aspect Ratio</th>\n",
              "      <th>Column Histogram 0</th>\n",
              "      <th>Column Histogram 1</th>\n",
              "      <th>Column Histogram 2</th>\n",
              "      <th>Column Histogram 3</th>\n",
              "      <th>Column Histogram 4</th>\n",
              "      <th>Column Histogram 5</th>\n",
              "      <th>Column Histogram 6</th>\n",
              "      <th>Column Histogram 7</th>\n",
              "      <th>...</th>\n",
              "      <th>HOG 630</th>\n",
              "      <th>HOG 631</th>\n",
              "      <th>HOG 632</th>\n",
              "      <th>HOG 633</th>\n",
              "      <th>HOG 634</th>\n",
              "      <th>HOG 635</th>\n",
              "      <th>HOG 636</th>\n",
              "      <th>HOG 637</th>\n",
              "      <th>HOG 638</th>\n",
              "      <th>HOG 639</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2728.000000</td>\n",
              "      <td>2728.000000</td>\n",
              "      <td>2728.000000</td>\n",
              "      <td>2728.000000</td>\n",
              "      <td>2728.000000</td>\n",
              "      <td>2728.000000</td>\n",
              "      <td>2728.000000</td>\n",
              "      <td>2728.000000</td>\n",
              "      <td>2728.000000</td>\n",
              "      <td>2728.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>2728.000000</td>\n",
              "      <td>2728.000000</td>\n",
              "      <td>2728.000000</td>\n",
              "      <td>2728.000000</td>\n",
              "      <td>2728.000000</td>\n",
              "      <td>2728.000000</td>\n",
              "      <td>2728.000000</td>\n",
              "      <td>2728.000000</td>\n",
              "      <td>2728.000000</td>\n",
              "      <td>2728.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.920930</td>\n",
              "      <td>0.793757</td>\n",
              "      <td>0.032806</td>\n",
              "      <td>0.075754</td>\n",
              "      <td>0.123485</td>\n",
              "      <td>0.187879</td>\n",
              "      <td>0.262204</td>\n",
              "      <td>0.337069</td>\n",
              "      <td>0.405489</td>\n",
              "      <td>0.451683</td>\n",
              "      <td>...</td>\n",
              "      <td>0.028547</td>\n",
              "      <td>0.002413</td>\n",
              "      <td>0.023749</td>\n",
              "      <td>0.009320</td>\n",
              "      <td>0.118547</td>\n",
              "      <td>0.027727</td>\n",
              "      <td>0.116288</td>\n",
              "      <td>0.015298</td>\n",
              "      <td>0.028397</td>\n",
              "      <td>0.002088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.574493</td>\n",
              "      <td>0.256534</td>\n",
              "      <td>0.079144</td>\n",
              "      <td>0.150864</td>\n",
              "      <td>0.201066</td>\n",
              "      <td>0.245763</td>\n",
              "      <td>0.282415</td>\n",
              "      <td>0.295835</td>\n",
              "      <td>0.295638</td>\n",
              "      <td>0.281953</td>\n",
              "      <td>...</td>\n",
              "      <td>0.065096</td>\n",
              "      <td>0.010147</td>\n",
              "      <td>0.056216</td>\n",
              "      <td>0.024494</td>\n",
              "      <td>0.103208</td>\n",
              "      <td>0.033364</td>\n",
              "      <td>0.122328</td>\n",
              "      <td>0.035007</td>\n",
              "      <td>0.055567</td>\n",
              "      <td>0.008343</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.250764</td>\n",
              "      <td>0.145482</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5%</th>\n",
              "      <td>0.416202</td>\n",
              "      <td>0.444991</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10%</th>\n",
              "      <td>0.474304</td>\n",
              "      <td>0.525757</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.613739</td>\n",
              "      <td>0.636770</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.050654</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.233333</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.789889</td>\n",
              "      <td>0.768109</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.039412</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.279020</td>\n",
              "      <td>0.366667</td>\n",
              "      <td>0.421111</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.117437</td>\n",
              "      <td>0.016410</td>\n",
              "      <td>0.076482</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.073029</td>\n",
              "      <td>0.919418</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.466667</td>\n",
              "      <td>0.566667</td>\n",
              "      <td>0.633333</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.011913</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.218082</td>\n",
              "      <td>0.045562</td>\n",
              "      <td>0.239293</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.036160</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90%</th>\n",
              "      <td>1.423110</td>\n",
              "      <td>1.106355</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.455556</td>\n",
              "      <td>0.566667</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.766667</td>\n",
              "      <td>0.860235</td>\n",
              "      <td>0.895961</td>\n",
              "      <td>...</td>\n",
              "      <td>0.126002</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.078463</td>\n",
              "      <td>0.033085</td>\n",
              "      <td>0.257358</td>\n",
              "      <td>0.072007</td>\n",
              "      <td>0.284325</td>\n",
              "      <td>0.064247</td>\n",
              "      <td>0.103458</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99%</th>\n",
              "      <td>2.853005</td>\n",
              "      <td>1.611474</td>\n",
              "      <td>0.353179</td>\n",
              "      <td>0.664448</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>0.966667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.262406</td>\n",
              "      <td>0.054255</td>\n",
              "      <td>0.258622</td>\n",
              "      <td>0.123749</td>\n",
              "      <td>0.297355</td>\n",
              "      <td>0.134428</td>\n",
              "      <td>0.440280</td>\n",
              "      <td>0.155178</td>\n",
              "      <td>0.245899</td>\n",
              "      <td>0.043373</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>10.400259</td>\n",
              "      <td>2.137500</td>\n",
              "      <td>0.831373</td>\n",
              "      <td>0.966667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.296387</td>\n",
              "      <td>0.129691</td>\n",
              "      <td>0.328205</td>\n",
              "      <td>0.259843</td>\n",
              "      <td>0.371676</td>\n",
              "      <td>0.252678</td>\n",
              "      <td>0.495138</td>\n",
              "      <td>0.312616</td>\n",
              "      <td>0.312497</td>\n",
              "      <td>0.079679</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>12 rows × 822 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-82f0079e-9cd0-4899-87a9-61bc2d552c8b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-82f0079e-9cd0-4899-87a9-61bc2d552c8b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-82f0079e-9cd0-4899-87a9-61bc2d552c8b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "ndataTrain.describe(percentiles = [0.05,0.10,0.25,0.50,0.75,0.90,0.99])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zo9h2ERr9j4F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b11ecf52-9ee2-4c30-819b-2ac121201466"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C',\n",
              "       'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P',\n",
              "       'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c',\n",
              "       'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p',\n",
              "       'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "np.unique(ndataTrain['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0LI8DDL9j8l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f962153-7bc1-4d9f-acac-e878d7557ff7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    44\n",
              "k    44\n",
              "X    44\n",
              "Y    44\n",
              "Z    44\n",
              "     ..\n",
              "P    44\n",
              "Q    44\n",
              "R    44\n",
              "S    44\n",
              "z    44\n",
              "Name: label, Length: 62, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "ndataTrain['label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOSVMDjg-MzD"
      },
      "outputs": [],
      "source": [
        "y_train = ndataTrain['label']\n",
        "y_eval = ndataEval['label']\n",
        "ndataTrain = ndataTrain.drop(\"label\", axis = 1)\n",
        "ndataTrain = ndataTrain.drop(\"image\", axis = 1)\n",
        "ndataEval = ndataEval.drop(\"label\", axis = 1)\n",
        "ndataEval = ndataEval.drop(\"image\", axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faipHi4Ob_yE"
      },
      "outputs": [],
      "source": [
        "X_train = ndataTrain\n",
        "X_eval = ndataEval\n",
        "# train test split\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7,test_size = 0.3, random_state = 101)\n",
        "# pca = PCA(n_components=50)\n",
        "# X_train = pca.fit_transform(X_train)\n",
        "# X_test = pca.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAV_ZYEjcV0H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ac662f8-8359-413d-92ab-0c9726f13856"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (2728, 822)\n",
            "y_train shape: (2728,)\n",
            "X_test shape: (682, 822)\n",
            "y_test shape: (682,)\n"
          ]
        }
      ],
      "source": [
        "print('X_train shape:',X_train.shape)\n",
        "print('y_train shape:',y_train.shape)\n",
        "print('X_test shape:',X_eval.shape)\n",
        "print('y_test shape:',y_eval.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuTyudCLcxe7"
      },
      "source": [
        "MODEL BUILDING"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "v = X_train.isnull().any()\n",
        "#print(v)\n",
        "print(v[v == True])"
      ],
      "metadata": {
        "id": "Zc73q3E0ajGC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca6687e6-0913-4eeb-9a3e-3d4f01d35e1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Series([], dtype: bool)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsMW2_KlcV6e"
      },
      "outputs": [],
      "source": [
        "model_linear = SVC(kernel='linear', probability = True)\n",
        "model_linear.fit(X_train, y_train)\n",
        "\n",
        "# predict\n",
        "y_pred = model_linear.predict_proba(X_eval)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model_linear.predict_proba(X_eval)\n",
        "print(str(len(y_pred[0])))"
      ],
      "metadata": {
        "id": "gMpyWxpti9Cm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1feabfed-72f6-44de-9e53-4c799c94df14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "predictions_proba = y_pred[10]\n",
        "def Top3(predictions_proba):\n",
        "\n",
        "  predictions_proba = predictions_proba.tolist()\n",
        "  lea = \"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\n",
        "  predictions=[]\n",
        "\n",
        "  index1 = predictions_proba.index(max(predictions_proba))\n",
        "  predictions.append(lea[index1])\n",
        "  predictions.append(predictions_proba[index1]) \n",
        "  predictions_proba[index1] = 0\n",
        "\n",
        "  index2 = predictions_proba.index(max(predictions_proba))\n",
        "  predictions.append(lea[index2])\n",
        "  predictions.append(predictions_proba[index2]) \n",
        "  predictions_proba[index2] = 0\n",
        "\n",
        "  index3 = predictions_proba.index(max(predictions_proba))\n",
        "  predictions.append(lea[index3])\n",
        "  predictions.append(predictions_proba[index3]) \n",
        "  predictions_proba[index3] = 0\n",
        " \n",
        "  return predictions_proba, predictions\n",
        "\n",
        "predictions_proba, predictions= Top3(predictions_proba)\n",
        "print(len(predictions_proba))\n",
        "print(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "Z3-Fjg9KgoeB",
        "outputId": "89cedff5-17ee-44f4-f731-7126d229b0a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-88-92565c606935>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mpredictions_proba\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mpredictions_proba\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mTop3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions_proba\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions_proba\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-88-92565c606935>\u001b[0m in \u001b[0;36mTop3\u001b[0;34m(predictions_proba)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mTop3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions_proba\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mpredictions_proba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions_proba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mlea\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'tolist'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVoBmOUBcV8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6498a932-d203-42fd-ace0-3c29473ce08d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.8299120234604106 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# accuracy\n",
        "print(\"accuracy:\", metrics.accuracy_score(y_true=y_eval, y_pred=y_pred), \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhD4ssEZdT0s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a6acf5b-88ca-4601-e57f-e2d9c3f20ff3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['0' '1' '2' '3' '4' '5' '6' '7' '8' '9' 'A' 'B' 'C' 'D' 'E' 'F' 'G' 'H'\n",
            " 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V' 'W' 'X' 'Y' 'Z'\n",
            " 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o' 'p' 'q' 'r'\n",
            " 's' 't' 'u' 'v' 'w' 'x' 'y' 'z']\n",
            "0    44\n",
            "k    44\n",
            "X    44\n",
            "Y    44\n",
            "Z    44\n",
            "     ..\n",
            "P    44\n",
            "Q    44\n",
            "R    44\n",
            "S    44\n",
            "z    44\n",
            "Name: label, Length: 62, dtype: int64\n",
            "0    11\n",
            "k    11\n",
            "X    11\n",
            "Y    11\n",
            "Z    11\n",
            "     ..\n",
            "P    11\n",
            "Q    11\n",
            "R    11\n",
            "S    11\n",
            "z    11\n",
            "Name: label, Length: 62, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(np.unique(y_train))\n",
        "print(y_train.value_counts())\n",
        "print(y_eval.value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqHkyyXieWgP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0c515c9-c747-47db-afb4-0abb5c1ce98e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 4  0  0 ...  0  0  0]\n",
            " [ 0  9  0 ...  0  0  0]\n",
            " [ 0  0 11 ...  0  0  0]\n",
            " ...\n",
            " [ 0  0  0 ...  8  0  0]\n",
            " [ 0  0  0 ...  0  9  0]\n",
            " [ 0  0  1 ...  0  0  6]]\n"
          ]
        }
      ],
      "source": [
        "# cm\n",
        "print(metrics.confusion_matrix(y_true=y_eval, y_pred=y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_aMKxvVe6kb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa017be6-0c2b-46f2-801a-48cb057de055"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.36      0.47        11\n",
            "           1       0.56      0.82      0.67        11\n",
            "           2       0.92      1.00      0.96        11\n",
            "           3       0.92      1.00      0.96        11\n",
            "           4       0.92      1.00      0.96        11\n",
            "           5       0.91      0.91      0.91        11\n",
            "           6       1.00      1.00      1.00        11\n",
            "           7       1.00      1.00      1.00        11\n",
            "           8       0.91      0.91      0.91        11\n",
            "           9       0.77      0.91      0.83        11\n",
            "           A       1.00      1.00      1.00        11\n",
            "           B       0.85      1.00      0.92        11\n",
            "           C       0.83      0.91      0.87        11\n",
            "           D       1.00      1.00      1.00        11\n",
            "           E       1.00      1.00      1.00        11\n",
            "           F       1.00      1.00      1.00        11\n",
            "           G       1.00      1.00      1.00        11\n",
            "           H       0.85      1.00      0.92        11\n",
            "           I       0.67      0.73      0.70        11\n",
            "           J       0.69      1.00      0.81        11\n",
            "           K       0.83      0.91      0.87        11\n",
            "           L       0.85      1.00      0.92        11\n",
            "           M       1.00      0.91      0.95        11\n",
            "           N       1.00      1.00      1.00        11\n",
            "           O       0.56      0.82      0.67        11\n",
            "           P       0.90      0.82      0.86        11\n",
            "           Q       0.92      1.00      0.96        11\n",
            "           R       0.90      0.82      0.86        11\n",
            "           S       0.62      0.73      0.67        11\n",
            "           T       1.00      0.91      0.95        11\n",
            "           U       0.83      0.91      0.87        11\n",
            "           V       0.80      0.73      0.76        11\n",
            "           W       0.69      0.82      0.75        11\n",
            "           X       0.83      0.91      0.87        11\n",
            "           Y       0.83      0.91      0.87        11\n",
            "           Z       0.89      0.73      0.80        11\n",
            "           a       1.00      0.91      0.95        11\n",
            "           b       1.00      1.00      1.00        11\n",
            "           c       0.80      0.73      0.76        11\n",
            "           d       0.85      1.00      0.92        11\n",
            "           e       0.90      0.82      0.86        11\n",
            "           f       0.71      0.91      0.80        11\n",
            "           g       0.70      0.64      0.67        11\n",
            "           h       0.79      1.00      0.88        11\n",
            "           i       0.00      0.00      0.00        11\n",
            "           j       1.00      0.36      0.53        11\n",
            "           k       0.80      0.73      0.76        11\n",
            "           l       0.58      0.64      0.61        11\n",
            "           m       0.92      1.00      0.96        11\n",
            "           n       0.90      0.82      0.86        11\n",
            "           o       0.55      0.55      0.55        11\n",
            "           p       0.70      0.64      0.67        11\n",
            "           q       0.82      0.82      0.82        11\n",
            "           r       0.64      0.82      0.72        11\n",
            "           s       0.78      0.64      0.70        11\n",
            "           t       0.90      0.82      0.86        11\n",
            "           u       0.90      0.82      0.86        11\n",
            "           v       0.78      0.64      0.70        11\n",
            "           w       0.78      0.64      0.70        11\n",
            "           x       1.00      0.73      0.84        11\n",
            "           y       0.82      0.82      0.82        11\n",
            "           z       0.67      0.55      0.60        11\n",
            "\n",
            "    accuracy                           0.83       682\n",
            "   macro avg       0.83      0.83      0.82       682\n",
            "weighted avg       0.83      0.83      0.82       682\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "#precision, recall and f1-score\n",
        "scores=metrics.classification_report(y_eval, y_pred)\n",
        "print(scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKycKNkSiQIJ"
      },
      "source": [
        "MODEL 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etrf46-EiOxj"
      },
      "outputs": [],
      "source": [
        "# non-linear model\n",
        "# using poly kernel, C=1, default value of gamma\n",
        "\n",
        "# model\n",
        "non_linear_model_poly = SVC(kernel='poly')\n",
        "\n",
        "# fit\n",
        "non_linear_model_poly.fit(X_train, y_train)\n",
        "\n",
        "# predict\n",
        "y_pred = non_linear_model_poly.predict(X_eval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8R6IejUPjKiU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d1f5d94-6f0f-45fc-d33e-da24d725fdfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.8181818181818182 \n",
            "\n",
            "[[ 4  0  0 ...  0  0  0]\n",
            " [ 0  6  0 ...  0  0  0]\n",
            " [ 0  0 11 ...  0  0  0]\n",
            " ...\n",
            " [ 0  0  0 ...  9  0  0]\n",
            " [ 0  0  0 ...  0  7  0]\n",
            " [ 0  0  1 ...  0  0  6]]\n"
          ]
        }
      ],
      "source": [
        "# confusion matrix and accuracy, precision, recall\n",
        "\n",
        "# accuracy\n",
        "print(\"accuracy:\", metrics.accuracy_score(y_true=y_eval, y_pred=y_pred), \"\\n\")\n",
        "\n",
        "# cm\n",
        "print(metrics.confusion_matrix(y_true=y_eval, y_pred=y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jajWyu4SjRcS"
      },
      "source": [
        "MODEL 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUk-bIARjTKn"
      },
      "outputs": [],
      "source": [
        "# non-linear model\n",
        "# using rbf kernel, C=1, default value of gamma\n",
        "\n",
        "# model\n",
        "non_linear_model = SVC(kernel='rbf')\n",
        "\n",
        "# fit\n",
        "non_linear_model.fit(X_train, y_train)\n",
        "\n",
        "# predict\n",
        "y_pred = non_linear_model.predict(X_eval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jUBS8YPjWhR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "930435f9-b239-447c-b0a9-f4329ba0ca37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.8167155425219942 \n",
            "\n",
            "[[ 2  0  0 ...  0  0  0]\n",
            " [ 0  8  0 ...  0  0  0]\n",
            " [ 0  0 11 ...  0  0  0]\n",
            " ...\n",
            " [ 0  0  0 ...  9  0  0]\n",
            " [ 0  0  0 ...  0  7  0]\n",
            " [ 0  1  2 ...  0  0  6]]\n"
          ]
        }
      ],
      "source": [
        "# confusion matrix and accuracy, precision, recall\n",
        "# accuracy\n",
        "print(\"accuracy:\", metrics.accuracy_score(y_true=y_eval, y_pred=y_pred), \"\\n\")\n",
        "\n",
        "# cm\n",
        "print(metrics.confusion_matrix(y_true=y_eval, y_pred=y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhPtdDbNje3H",
        "outputId": "c63de0d6-9e39-43e0-e973-07c6f7010f9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.18      0.27        11\n",
            "           1       0.42      0.73      0.53        11\n",
            "           2       0.79      1.00      0.88        11\n",
            "           3       0.83      0.91      0.87        11\n",
            "           4       0.79      1.00      0.88        11\n",
            "           5       0.89      0.73      0.80        11\n",
            "           6       0.92      1.00      0.96        11\n",
            "           7       1.00      0.91      0.95        11\n",
            "           8       0.83      0.91      0.87        11\n",
            "           9       0.64      0.82      0.72        11\n",
            "           A       0.92      1.00      0.96        11\n",
            "           B       1.00      1.00      1.00        11\n",
            "           C       0.71      0.91      0.80        11\n",
            "           D       1.00      1.00      1.00        11\n",
            "           E       1.00      1.00      1.00        11\n",
            "           F       1.00      1.00      1.00        11\n",
            "           G       1.00      0.91      0.95        11\n",
            "           H       1.00      1.00      1.00        11\n",
            "           I       0.71      0.45      0.56        11\n",
            "           J       0.85      1.00      0.92        11\n",
            "           K       0.85      1.00      0.92        11\n",
            "           L       0.77      0.91      0.83        11\n",
            "           M       1.00      0.82      0.90        11\n",
            "           N       0.92      1.00      0.96        11\n",
            "           O       0.56      0.82      0.67        11\n",
            "           P       0.82      0.82      0.82        11\n",
            "           Q       0.82      0.82      0.82        11\n",
            "           R       1.00      0.82      0.90        11\n",
            "           S       0.64      0.82      0.72        11\n",
            "           T       1.00      0.91      0.95        11\n",
            "           U       1.00      0.91      0.95        11\n",
            "           V       0.80      0.73      0.76        11\n",
            "           W       0.75      0.82      0.78        11\n",
            "           X       0.91      0.91      0.91        11\n",
            "           Y       0.79      1.00      0.88        11\n",
            "           Z       1.00      0.82      0.90        11\n",
            "           a       0.90      0.82      0.86        11\n",
            "           b       1.00      1.00      1.00        11\n",
            "           c       0.83      0.45      0.59        11\n",
            "           d       1.00      1.00      1.00        11\n",
            "           e       0.89      0.73      0.80        11\n",
            "           f       0.77      0.91      0.83        11\n",
            "           g       0.64      0.64      0.64        11\n",
            "           h       0.92      1.00      0.96        11\n",
            "           i       0.00      0.00      0.00        11\n",
            "           j       1.00      0.73      0.84        11\n",
            "           k       1.00      0.82      0.90        11\n",
            "           l       0.50      0.64      0.56        11\n",
            "           m       0.92      1.00      0.96        11\n",
            "           n       0.92      1.00      0.96        11\n",
            "           o       0.46      0.55      0.50        11\n",
            "           p       0.67      0.55      0.60        11\n",
            "           q       0.78      0.64      0.70        11\n",
            "           r       0.69      0.82      0.75        11\n",
            "           s       0.67      0.73      0.70        11\n",
            "           t       1.00      0.91      0.95        11\n",
            "           u       0.92      1.00      0.96        11\n",
            "           v       0.78      0.64      0.70        11\n",
            "           w       0.80      0.73      0.76        11\n",
            "           x       0.82      0.82      0.82        11\n",
            "           y       0.70      0.64      0.67        11\n",
            "           z       0.60      0.55      0.57        11\n",
            "\n",
            "    accuracy                           0.82       682\n",
            "   macro avg       0.82      0.82      0.81       682\n",
            "weighted avg       0.82      0.82      0.81       682\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#precision, recall and f1-score\n",
        "scores=metrics.classification_report(y_eval, y_pred)\n",
        "print(scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbnDxRDLjnNg"
      },
      "source": [
        "Grid Search: Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SVM\n",
        "hyper_param = {'C': [0.1, 1, 10, 100], \n",
        "               'gamma': [1, 0.1, 0.01, 0.001], \n",
        "               'kernel': ['linear', 'rbf', 'poly']}\n",
        "model = SVC(probability = True)\n",
        "gs = GridSearchCV(estimator=model,\n",
        "                    param_grid=hyper_param,\n",
        "                    cv=StratifiedKFold(5).split(X_train, y_train),\n",
        "                    scoring='accuracy',\n",
        "                    refit=True,\n",
        "                    verbose=3)\n",
        "\n",
        "gs.fit(X_train, y_train)\n",
        "\n",
        "# save the model to disk\n",
        "joblib.dump(gs.best_estimator_, 'FinalSVCProfileWhite.npy')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LBWOvOEtOBG",
        "outputId": "124ef98c-3650-45b8-b468-1abc62a545f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
            "[CV 1/5] END .....C=0.1, gamma=1, kernel=linear;, score=0.799 total time=   5.1s\n",
            "[CV 2/5] END .....C=0.1, gamma=1, kernel=linear;, score=0.853 total time=   6.4s\n",
            "[CV 3/5] END .....C=0.1, gamma=1, kernel=linear;, score=0.835 total time=   5.1s\n",
            "[CV 4/5] END .....C=0.1, gamma=1, kernel=linear;, score=0.828 total time=   5.1s\n",
            "[CV 5/5] END .....C=0.1, gamma=1, kernel=linear;, score=0.771 total time=   4.9s\n",
            "[CV 1/5] END ........C=0.1, gamma=1, kernel=rbf;, score=0.084 total time=  16.6s\n",
            "[CV 2/5] END ........C=0.1, gamma=1, kernel=rbf;, score=0.084 total time=  17.1s\n",
            "[CV 3/5] END ........C=0.1, gamma=1, kernel=rbf;, score=0.088 total time=  21.1s\n",
            "[CV 4/5] END ........C=0.1, gamma=1, kernel=rbf;, score=0.119 total time=  15.2s\n",
            "[CV 5/5] END ........C=0.1, gamma=1, kernel=rbf;, score=0.094 total time=  16.1s\n",
            "[CV 1/5] END .......C=0.1, gamma=1, kernel=poly;, score=0.793 total time=   4.9s\n",
            "[CV 2/5] END .......C=0.1, gamma=1, kernel=poly;, score=0.839 total time=   4.9s\n",
            "[CV 3/5] END .......C=0.1, gamma=1, kernel=poly;, score=0.842 total time=   4.9s\n",
            "[CV 4/5] END .......C=0.1, gamma=1, kernel=poly;, score=0.831 total time=   4.8s\n",
            "[CV 5/5] END .......C=0.1, gamma=1, kernel=poly;, score=0.750 total time=   4.8s\n",
            "[CV 1/5] END ...C=0.1, gamma=0.1, kernel=linear;, score=0.799 total time=   5.0s\n",
            "[CV 2/5] END ...C=0.1, gamma=0.1, kernel=linear;, score=0.853 total time=   5.1s\n",
            "[CV 3/5] END ...C=0.1, gamma=0.1, kernel=linear;, score=0.835 total time=   5.0s\n",
            "[CV 4/5] END ...C=0.1, gamma=0.1, kernel=linear;, score=0.828 total time=   5.0s\n",
            "[CV 5/5] END ...C=0.1, gamma=0.1, kernel=linear;, score=0.771 total time=   5.1s\n",
            "[CV 1/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.518 total time=  14.4s\n",
            "[CV 2/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.571 total time=  14.7s\n",
            "[CV 3/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.588 total time=  14.9s\n",
            "[CV 4/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.587 total time=  14.9s\n",
            "[CV 5/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.534 total time=  14.7s\n",
            "[CV 1/5] END .....C=0.1, gamma=0.1, kernel=poly;, score=0.808 total time=   5.4s\n",
            "[CV 2/5] END .....C=0.1, gamma=0.1, kernel=poly;, score=0.844 total time=   5.3s\n",
            "[CV 3/5] END .....C=0.1, gamma=0.1, kernel=poly;, score=0.826 total time=   5.2s\n",
            "[CV 4/5] END .....C=0.1, gamma=0.1, kernel=poly;, score=0.813 total time=   5.1s\n",
            "[CV 5/5] END .....C=0.1, gamma=0.1, kernel=poly;, score=0.750 total time=   5.1s\n",
            "[CV 1/5] END ..C=0.1, gamma=0.01, kernel=linear;, score=0.799 total time=   5.0s\n",
            "[CV 2/5] END ..C=0.1, gamma=0.01, kernel=linear;, score=0.853 total time=   5.2s\n",
            "[CV 3/5] END ..C=0.1, gamma=0.01, kernel=linear;, score=0.835 total time=   5.2s\n",
            "[CV 4/5] END ..C=0.1, gamma=0.01, kernel=linear;, score=0.828 total time=   5.1s\n",
            "[CV 5/5] END ..C=0.1, gamma=0.01, kernel=linear;, score=0.771 total time=   5.1s\n",
            "[CV 1/5] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.148 total time=  14.8s\n",
            "[CV 2/5] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.156 total time=  14.6s\n",
            "[CV 3/5] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.158 total time=  14.6s\n",
            "[CV 4/5] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.161 total time=  14.7s\n",
            "[CV 5/5] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.169 total time=  14.5s\n",
            "[CV 1/5] END ....C=0.1, gamma=0.01, kernel=poly;, score=0.119 total time=  10.3s\n",
            "[CV 2/5] END ....C=0.1, gamma=0.01, kernel=poly;, score=0.104 total time=  10.2s\n",
            "[CV 3/5] END ....C=0.1, gamma=0.01, kernel=poly;, score=0.123 total time=  10.1s\n",
            "[CV 4/5] END ....C=0.1, gamma=0.01, kernel=poly;, score=0.084 total time=  10.4s\n",
            "[CV 5/5] END ....C=0.1, gamma=0.01, kernel=poly;, score=0.117 total time=  10.3s\n",
            "[CV 1/5] END .C=0.1, gamma=0.001, kernel=linear;, score=0.799 total time=   5.0s\n",
            "[CV 2/5] END .C=0.1, gamma=0.001, kernel=linear;, score=0.853 total time=   5.1s\n",
            "[CV 3/5] END .C=0.1, gamma=0.001, kernel=linear;, score=0.835 total time=   5.3s\n",
            "[CV 4/5] END .C=0.1, gamma=0.001, kernel=linear;, score=0.828 total time=   5.1s\n",
            "[CV 5/5] END .C=0.1, gamma=0.001, kernel=linear;, score=0.771 total time=   5.0s\n",
            "[CV 1/5] END ....C=0.1, gamma=0.001, kernel=rbf;, score=0.148 total time=  14.8s\n",
            "[CV 2/5] END ....C=0.1, gamma=0.001, kernel=rbf;, score=0.156 total time=  14.9s\n",
            "[CV 3/5] END ....C=0.1, gamma=0.001, kernel=rbf;, score=0.156 total time=  14.8s\n",
            "[CV 4/5] END ....C=0.1, gamma=0.001, kernel=rbf;, score=0.161 total time=  15.1s\n",
            "[CV 5/5] END ....C=0.1, gamma=0.001, kernel=rbf;, score=0.167 total time=  14.4s\n",
            "[CV 1/5] END ...C=0.1, gamma=0.001, kernel=poly;, score=0.119 total time=  10.4s\n",
            "[CV 2/5] END ...C=0.1, gamma=0.001, kernel=poly;, score=0.104 total time=  10.4s\n",
            "[CV 3/5] END ...C=0.1, gamma=0.001, kernel=poly;, score=0.123 total time=  10.2s\n",
            "[CV 4/5] END ...C=0.1, gamma=0.001, kernel=poly;, score=0.084 total time=  10.3s\n",
            "[CV 5/5] END ...C=0.1, gamma=0.001, kernel=poly;, score=0.117 total time=  10.4s\n",
            "[CV 1/5] END .......C=1, gamma=1, kernel=linear;, score=0.806 total time=   4.5s\n",
            "[CV 2/5] END .......C=1, gamma=1, kernel=linear;, score=0.852 total time=   4.6s\n",
            "[CV 3/5] END .......C=1, gamma=1, kernel=linear;, score=0.848 total time=   4.6s\n",
            "[CV 4/5] END .......C=1, gamma=1, kernel=linear;, score=0.833 total time=   4.6s\n",
            "[CV 5/5] END .......C=1, gamma=1, kernel=linear;, score=0.769 total time=   4.6s\n",
            "[CV 1/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.482 total time=  15.7s\n",
            "[CV 2/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.689 total time=  15.6s\n",
            "[CV 3/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.701 total time=  15.5s\n",
            "[CV 4/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.668 total time=  15.8s\n",
            "[CV 5/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.589 total time=  15.6s\n",
            "[CV 1/5] END .........C=1, gamma=1, kernel=poly;, score=0.793 total time=   4.9s\n",
            "[CV 2/5] END .........C=1, gamma=1, kernel=poly;, score=0.839 total time=   5.0s\n",
            "[CV 3/5] END .........C=1, gamma=1, kernel=poly;, score=0.842 total time=   4.8s\n",
            "[CV 4/5] END .........C=1, gamma=1, kernel=poly;, score=0.831 total time=   4.8s\n",
            "[CV 5/5] END .........C=1, gamma=1, kernel=poly;, score=0.750 total time=   4.7s\n",
            "[CV 1/5] END .....C=1, gamma=0.1, kernel=linear;, score=0.806 total time=   4.6s\n",
            "[CV 2/5] END .....C=1, gamma=0.1, kernel=linear;, score=0.852 total time=   4.7s\n",
            "[CV 3/5] END .....C=1, gamma=0.1, kernel=linear;, score=0.848 total time=   4.7s\n",
            "[CV 4/5] END .....C=1, gamma=0.1, kernel=linear;, score=0.833 total time=   4.6s\n",
            "[CV 5/5] END .....C=1, gamma=0.1, kernel=linear;, score=0.769 total time=   4.6s\n",
            "[CV 1/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.791 total time=   9.9s\n",
            "[CV 2/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.861 total time=  10.5s\n",
            "[CV 3/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.846 total time=  10.9s\n",
            "[CV 4/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.844 total time=  10.2s\n",
            "[CV 5/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.787 total time=  10.1s\n",
            "[CV 1/5] END .......C=1, gamma=0.1, kernel=poly;, score=0.800 total time=   5.0s\n",
            "[CV 2/5] END .......C=1, gamma=0.1, kernel=poly;, score=0.848 total time=   5.0s\n",
            "[CV 3/5] END .......C=1, gamma=0.1, kernel=poly;, score=0.852 total time=   5.0s\n",
            "[CV 4/5] END .......C=1, gamma=0.1, kernel=poly;, score=0.831 total time=   4.7s\n",
            "[CV 5/5] END .......C=1, gamma=0.1, kernel=poly;, score=0.772 total time=   4.8s\n",
            "[CV 1/5] END ....C=1, gamma=0.01, kernel=linear;, score=0.806 total time=   4.5s\n",
            "[CV 2/5] END ....C=1, gamma=0.01, kernel=linear;, score=0.852 total time=   4.6s\n",
            "[CV 3/5] END ....C=1, gamma=0.01, kernel=linear;, score=0.848 total time=   4.7s\n",
            "[CV 4/5] END ....C=1, gamma=0.01, kernel=linear;, score=0.833 total time=   4.6s\n",
            "[CV 5/5] END ....C=1, gamma=0.01, kernel=linear;, score=0.769 total time=   4.5s\n",
            "[CV 1/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.689 total time=  11.8s\n",
            "[CV 2/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.745 total time=  12.0s\n",
            "[CV 3/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.714 total time=  11.8s\n",
            "[CV 4/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.730 total time=  11.6s\n",
            "[CV 5/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.651 total time=  11.6s\n",
            "[CV 1/5] END ......C=1, gamma=0.01, kernel=poly;, score=0.121 total time=  10.2s\n",
            "[CV 2/5] END ......C=1, gamma=0.01, kernel=poly;, score=0.108 total time=  10.3s\n",
            "[CV 3/5] END ......C=1, gamma=0.01, kernel=poly;, score=0.125 total time=  10.4s\n",
            "[CV 4/5] END ......C=1, gamma=0.01, kernel=poly;, score=0.086 total time=  10.3s\n",
            "[CV 5/5] END ......C=1, gamma=0.01, kernel=poly;, score=0.123 total time=  10.5s\n",
            "[CV 1/5] END ...C=1, gamma=0.001, kernel=linear;, score=0.806 total time=   4.6s\n",
            "[CV 2/5] END ...C=1, gamma=0.001, kernel=linear;, score=0.852 total time=   4.6s\n",
            "[CV 3/5] END ...C=1, gamma=0.001, kernel=linear;, score=0.848 total time=   4.6s\n",
            "[CV 4/5] END ...C=1, gamma=0.001, kernel=linear;, score=0.833 total time=   4.6s\n",
            "[CV 5/5] END ...C=1, gamma=0.001, kernel=linear;, score=0.769 total time=   4.5s\n",
            "[CV 1/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.148 total time=  14.4s\n",
            "[CV 2/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.156 total time=  14.9s\n",
            "[CV 3/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.156 total time=  14.8s\n",
            "[CV 4/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.161 total time=  14.5s\n",
            "[CV 5/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.167 total time=  14.6s\n",
            "[CV 1/5] END .....C=1, gamma=0.001, kernel=poly;, score=0.119 total time=  10.4s\n",
            "[CV 2/5] END .....C=1, gamma=0.001, kernel=poly;, score=0.104 total time=  10.3s\n",
            "[CV 3/5] END .....C=1, gamma=0.001, kernel=poly;, score=0.123 total time=  10.3s\n",
            "[CV 4/5] END .....C=1, gamma=0.001, kernel=poly;, score=0.084 total time=  10.6s\n",
            "[CV 5/5] END .....C=1, gamma=0.001, kernel=poly;, score=0.117 total time=  10.4s\n",
            "[CV 1/5] END ......C=10, gamma=1, kernel=linear;, score=0.797 total time=   4.5s\n",
            "[CV 2/5] END ......C=10, gamma=1, kernel=linear;, score=0.839 total time=   4.6s\n",
            "[CV 3/5] END ......C=10, gamma=1, kernel=linear;, score=0.837 total time=   4.7s\n",
            "[CV 4/5] END ......C=10, gamma=1, kernel=linear;, score=0.822 total time=   4.8s\n",
            "[CV 5/5] END ......C=10, gamma=1, kernel=linear;, score=0.756 total time=   4.6s\n",
            "[CV 1/5] END .........C=10, gamma=1, kernel=rbf;, score=0.515 total time=  15.8s\n",
            "[CV 2/5] END .........C=10, gamma=1, kernel=rbf;, score=0.705 total time=  15.4s\n",
            "[CV 3/5] END .........C=10, gamma=1, kernel=rbf;, score=0.714 total time=  15.9s\n",
            "[CV 4/5] END .........C=10, gamma=1, kernel=rbf;, score=0.697 total time=  15.5s\n",
            "[CV 5/5] END .........C=10, gamma=1, kernel=rbf;, score=0.602 total time=  15.6s\n",
            "[CV 1/5] END ........C=10, gamma=1, kernel=poly;, score=0.793 total time=   4.8s\n",
            "[CV 2/5] END ........C=10, gamma=1, kernel=poly;, score=0.839 total time=   5.0s\n",
            "[CV 3/5] END ........C=10, gamma=1, kernel=poly;, score=0.842 total time=   5.0s\n",
            "[CV 4/5] END ........C=10, gamma=1, kernel=poly;, score=0.831 total time=   4.9s\n",
            "[CV 5/5] END ........C=10, gamma=1, kernel=poly;, score=0.750 total time=   4.9s\n",
            "[CV 1/5] END ....C=10, gamma=0.1, kernel=linear;, score=0.797 total time=   4.8s\n",
            "[CV 2/5] END ....C=10, gamma=0.1, kernel=linear;, score=0.839 total time=   4.8s\n",
            "[CV 3/5] END ....C=10, gamma=0.1, kernel=linear;, score=0.837 total time=   4.7s\n",
            "[CV 4/5] END ....C=10, gamma=0.1, kernel=linear;, score=0.822 total time=   4.6s\n",
            "[CV 5/5] END ....C=10, gamma=0.1, kernel=linear;, score=0.756 total time=   4.7s\n",
            "[CV 1/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.808 total time=  10.2s\n",
            "[CV 2/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.852 total time=  10.4s\n",
            "[CV 3/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.853 total time=  10.6s\n",
            "[CV 4/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.835 total time=  10.5s\n",
            "[CV 5/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.772 total time=  10.4s\n",
            "[CV 1/5] END ......C=10, gamma=0.1, kernel=poly;, score=0.793 total time=   4.8s\n",
            "[CV 2/5] END ......C=10, gamma=0.1, kernel=poly;, score=0.839 total time=   4.9s\n",
            "[CV 3/5] END ......C=10, gamma=0.1, kernel=poly;, score=0.842 total time=   4.9s\n",
            "[CV 4/5] END ......C=10, gamma=0.1, kernel=poly;, score=0.831 total time=   4.8s\n",
            "[CV 5/5] END ......C=10, gamma=0.1, kernel=poly;, score=0.750 total time=   4.7s\n",
            "[CV 1/5] END ...C=10, gamma=0.01, kernel=linear;, score=0.797 total time=   4.5s\n",
            "[CV 2/5] END ...C=10, gamma=0.01, kernel=linear;, score=0.839 total time=   4.6s\n",
            "[CV 3/5] END ...C=10, gamma=0.01, kernel=linear;, score=0.837 total time=   4.8s\n",
            "[CV 4/5] END ...C=10, gamma=0.01, kernel=linear;, score=0.822 total time=   4.7s\n",
            "[CV 5/5] END ...C=10, gamma=0.01, kernel=linear;, score=0.756 total time=   4.5s\n",
            "[CV 1/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.811 total time=   6.9s\n",
            "[CV 2/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.859 total time=   7.1s\n",
            "[CV 3/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.852 total time=   6.9s\n",
            "[CV 4/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.839 total time=   6.8s\n",
            "[CV 5/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.783 total time=   6.9s\n",
            "[CV 1/5] END .....C=10, gamma=0.01, kernel=poly;, score=0.592 total time=   8.8s\n",
            "[CV 2/5] END .....C=10, gamma=0.01, kernel=poly;, score=0.658 total time=   8.9s\n",
            "[CV 3/5] END .....C=10, gamma=0.01, kernel=poly;, score=0.597 total time=   8.9s\n",
            "[CV 4/5] END .....C=10, gamma=0.01, kernel=poly;, score=0.552 total time=   8.9s\n",
            "[CV 5/5] END .....C=10, gamma=0.01, kernel=poly;, score=0.554 total time=   9.1s\n",
            "[CV 1/5] END ..C=10, gamma=0.001, kernel=linear;, score=0.797 total time=   4.6s\n",
            "[CV 2/5] END ..C=10, gamma=0.001, kernel=linear;, score=0.839 total time=   4.6s\n",
            "[CV 3/5] END ..C=10, gamma=0.001, kernel=linear;, score=0.837 total time=   4.6s\n",
            "[CV 4/5] END ..C=10, gamma=0.001, kernel=linear;, score=0.822 total time=   4.7s\n",
            "[CV 5/5] END ..C=10, gamma=0.001, kernel=linear;, score=0.756 total time=   4.6s\n",
            "[CV 1/5] END .....C=10, gamma=0.001, kernel=rbf;, score=0.683 total time=  11.3s\n",
            "[CV 2/5] END .....C=10, gamma=0.001, kernel=rbf;, score=0.751 total time=  11.3s\n",
            "[CV 3/5] END .....C=10, gamma=0.001, kernel=rbf;, score=0.712 total time=  11.6s\n",
            "[CV 4/5] END .....C=10, gamma=0.001, kernel=rbf;, score=0.732 total time=  11.3s\n",
            "[CV 5/5] END .....C=10, gamma=0.001, kernel=rbf;, score=0.666 total time=  11.3s\n",
            "[CV 1/5] END ....C=10, gamma=0.001, kernel=poly;, score=0.119 total time=  10.0s\n",
            "[CV 2/5] END ....C=10, gamma=0.001, kernel=poly;, score=0.104 total time=  10.1s\n",
            "[CV 3/5] END ....C=10, gamma=0.001, kernel=poly;, score=0.123 total time=  10.3s\n",
            "[CV 4/5] END ....C=10, gamma=0.001, kernel=poly;, score=0.084 total time=  10.3s\n",
            "[CV 5/5] END ....C=10, gamma=0.001, kernel=poly;, score=0.117 total time=  10.4s\n",
            "[CV 1/5] END .....C=100, gamma=1, kernel=linear;, score=0.793 total time=   4.5s\n",
            "[CV 2/5] END .....C=100, gamma=1, kernel=linear;, score=0.839 total time=   4.9s\n",
            "[CV 3/5] END .....C=100, gamma=1, kernel=linear;, score=0.837 total time=   4.6s\n",
            "[CV 4/5] END .....C=100, gamma=1, kernel=linear;, score=0.826 total time=   4.6s\n",
            "[CV 5/5] END .....C=100, gamma=1, kernel=linear;, score=0.752 total time=   4.6s\n",
            "[CV 1/5] END ........C=100, gamma=1, kernel=rbf;, score=0.515 total time=  16.0s\n",
            "[CV 2/5] END ........C=100, gamma=1, kernel=rbf;, score=0.705 total time=  15.6s\n",
            "[CV 3/5] END ........C=100, gamma=1, kernel=rbf;, score=0.714 total time=  15.6s\n",
            "[CV 4/5] END ........C=100, gamma=1, kernel=rbf;, score=0.697 total time=  15.9s\n",
            "[CV 5/5] END ........C=100, gamma=1, kernel=rbf;, score=0.602 total time=  15.7s\n",
            "[CV 1/5] END .......C=100, gamma=1, kernel=poly;, score=0.793 total time=   4.8s\n",
            "[CV 2/5] END .......C=100, gamma=1, kernel=poly;, score=0.839 total time=   4.9s\n",
            "[CV 3/5] END .......C=100, gamma=1, kernel=poly;, score=0.842 total time=   5.0s\n",
            "[CV 4/5] END .......C=100, gamma=1, kernel=poly;, score=0.831 total time=   4.8s\n",
            "[CV 5/5] END .......C=100, gamma=1, kernel=poly;, score=0.750 total time=   4.8s\n",
            "[CV 1/5] END ...C=100, gamma=0.1, kernel=linear;, score=0.793 total time=   4.5s\n",
            "[CV 2/5] END ...C=100, gamma=0.1, kernel=linear;, score=0.839 total time=   4.8s\n",
            "[CV 3/5] END ...C=100, gamma=0.1, kernel=linear;, score=0.837 total time=   4.9s\n",
            "[CV 4/5] END ...C=100, gamma=0.1, kernel=linear;, score=0.826 total time=   4.7s\n",
            "[CV 5/5] END ...C=100, gamma=0.1, kernel=linear;, score=0.752 total time=   4.6s\n",
            "[CV 1/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.810 total time=  10.3s\n",
            "[CV 2/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.842 total time=  10.4s\n",
            "[CV 3/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.842 total time=  10.5s\n",
            "[CV 4/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.833 total time=  10.5s\n",
            "[CV 5/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.758 total time=  10.2s\n",
            "[CV 1/5] END .....C=100, gamma=0.1, kernel=poly;, score=0.793 total time=   4.9s\n",
            "[CV 2/5] END .....C=100, gamma=0.1, kernel=poly;, score=0.839 total time=   5.2s\n",
            "[CV 3/5] END .....C=100, gamma=0.1, kernel=poly;, score=0.842 total time=   5.4s\n",
            "[CV 4/5] END .....C=100, gamma=0.1, kernel=poly;, score=0.831 total time=   4.9s\n",
            "[CV 5/5] END .....C=100, gamma=0.1, kernel=poly;, score=0.750 total time=   4.8s\n",
            "[CV 1/5] END ..C=100, gamma=0.01, kernel=linear;, score=0.793 total time=   4.5s\n",
            "[CV 2/5] END ..C=100, gamma=0.01, kernel=linear;, score=0.839 total time=   4.6s\n",
            "[CV 3/5] END ..C=100, gamma=0.01, kernel=linear;, score=0.837 total time=   4.6s\n",
            "[CV 4/5] END ..C=100, gamma=0.01, kernel=linear;, score=0.826 total time=   4.7s\n",
            "[CV 5/5] END ..C=100, gamma=0.01, kernel=linear;, score=0.752 total time=   4.5s\n",
            "[CV 1/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.808 total time=   6.8s\n",
            "[CV 2/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.841 total time=   6.8s\n",
            "[CV 3/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.855 total time=   6.8s\n",
            "[CV 4/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.824 total time=   6.7s\n",
            "[CV 5/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.772 total time=   6.6s\n",
            "[CV 1/5] END ....C=100, gamma=0.01, kernel=poly;, score=0.808 total time=   5.1s\n",
            "[CV 2/5] END ....C=100, gamma=0.01, kernel=poly;, score=0.844 total time=   5.2s\n",
            "[CV 3/5] END ....C=100, gamma=0.01, kernel=poly;, score=0.826 total time=   5.2s\n",
            "[CV 4/5] END ....C=100, gamma=0.01, kernel=poly;, score=0.813 total time=   5.0s\n",
            "[CV 5/5] END ....C=100, gamma=0.01, kernel=poly;, score=0.750 total time=   5.0s\n",
            "[CV 1/5] END .C=100, gamma=0.001, kernel=linear;, score=0.793 total time=   4.6s\n",
            "[CV 2/5] END .C=100, gamma=0.001, kernel=linear;, score=0.839 total time=   4.6s\n",
            "[CV 3/5] END .C=100, gamma=0.001, kernel=linear;, score=0.837 total time=   4.6s\n",
            "[CV 4/5] END .C=100, gamma=0.001, kernel=linear;, score=0.826 total time=   4.6s\n",
            "[CV 5/5] END .C=100, gamma=0.001, kernel=linear;, score=0.752 total time=   4.5s\n",
            "[CV 1/5] END ....C=100, gamma=0.001, kernel=rbf;, score=0.813 total time=   6.6s\n",
            "[CV 2/5] END ....C=100, gamma=0.001, kernel=rbf;, score=0.861 total time=   6.7s\n",
            "[CV 3/5] END ....C=100, gamma=0.001, kernel=rbf;, score=0.853 total time=   6.7s\n",
            "[CV 4/5] END ....C=100, gamma=0.001, kernel=rbf;, score=0.839 total time=   6.6s\n",
            "[CV 5/5] END ....C=100, gamma=0.001, kernel=rbf;, score=0.782 total time=   6.5s\n",
            "[CV 1/5] END ...C=100, gamma=0.001, kernel=poly;, score=0.119 total time=  10.4s\n",
            "[CV 2/5] END ...C=100, gamma=0.001, kernel=poly;, score=0.104 total time=  10.2s\n",
            "[CV 3/5] END ...C=100, gamma=0.001, kernel=poly;, score=0.123 total time=  10.4s\n",
            "[CV 4/5] END ...C=100, gamma=0.001, kernel=poly;, score=0.084 total time=  10.3s\n",
            "[CV 5/5] END ...C=100, gamma=0.001, kernel=poly;, score=0.117 total time=  10.3s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['FinalSVCProfileWhite.npy']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l = 'Best Params of SVC with Profile White: '+str(gs.best_params_)+'\\n'\n",
        "l2 = 'Best Accuracy of SVC with Profile White: '+str(gs.best_score_)+'\\n'\n",
        "print(l)\n",
        "print(l2)\n",
        "y_pred = gs.predict(X_eval)\n",
        "\n",
        "# metrics\n",
        "print(\"EVAL accuracy\", metrics.accuracy_score(y_eval, y_pred), \"\\n\")\n",
        "print(metrics.confusion_matrix(y_eval, y_pred), \"\\n\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYs955QeUkVE",
        "outputId": "7614c801-376d-43ca-e8fc-2dada4f5f25a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Params of SVC with Profile White: {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "\n",
            "Best Accuracy of SVC with Profile White: 0.8295312027422119\n",
            "\n",
            "EVAL accuracy 0.8284457478005866 \n",
            "\n",
            "[[ 4  0  0 ...  0  0  0]\n",
            " [ 0  9  0 ...  0  0  0]\n",
            " [ 0  0 11 ...  0  0  0]\n",
            " ...\n",
            " [ 0  0  0 ...  9  0  0]\n",
            " [ 0  0  0 ...  0  9  0]\n",
            " [ 0  1  1 ...  0  0  6]] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#KNN\n",
        "from sklearn import neighbors\n",
        "knn = neighbors.KNeighborsClassifier()\n",
        "k_range = list(range(1, 31))\n",
        "param_grid = dict(n_neighbors=k_range)\n",
        "  \n",
        "# defining parameter range\n",
        "gsKNN = GridSearchCV(knn, \n",
        "                    param_grid, \n",
        "                    cv=StratifiedKFold(5).split(X_train, y_train), \n",
        "                    scoring='accuracy',\n",
        "                    verbose=3)\n",
        "  \n",
        "# fitting the model for grid search\n",
        "grid_search=gsKNN.fit(X_train, y_train)\n",
        "\n",
        "# save the model to disk\n",
        "joblib.dump(gsKNN.best_estimator_, 'BestKNNProfileWhite.npy')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_3LFWSyGStN",
        "outputId": "6bcc1758-6fc8-4cd6-d918-bcd34bff9277"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
            "[CV 1/5] END .....................n_neighbors=1;, score=0.698 total time=   0.1s\n",
            "[CV 2/5] END .....................n_neighbors=1;, score=0.808 total time=   0.1s\n",
            "[CV 3/5] END .....................n_neighbors=1;, score=0.775 total time=   0.1s\n",
            "[CV 4/5] END .....................n_neighbors=1;, score=0.767 total time=   0.2s\n",
            "[CV 5/5] END .....................n_neighbors=1;, score=0.701 total time=   0.2s\n",
            "[CV 1/5] END .....................n_neighbors=2;, score=0.698 total time=   0.2s\n",
            "[CV 2/5] END .....................n_neighbors=2;, score=0.771 total time=   0.1s\n",
            "[CV 3/5] END .....................n_neighbors=2;, score=0.753 total time=   0.1s\n",
            "[CV 4/5] END .....................n_neighbors=2;, score=0.730 total time=   0.1s\n",
            "[CV 5/5] END .....................n_neighbors=2;, score=0.651 total time=   0.2s\n",
            "[CV 1/5] END .....................n_neighbors=3;, score=0.718 total time=   0.2s\n",
            "[CV 2/5] END .....................n_neighbors=3;, score=0.797 total time=   0.2s\n",
            "[CV 3/5] END .....................n_neighbors=3;, score=0.780 total time=   0.2s\n",
            "[CV 4/5] END .....................n_neighbors=3;, score=0.745 total time=   0.2s\n",
            "[CV 5/5] END .....................n_neighbors=3;, score=0.697 total time=   0.1s\n",
            "[CV 1/5] END .....................n_neighbors=4;, score=0.714 total time=   0.2s\n",
            "[CV 2/5] END .....................n_neighbors=4;, score=0.797 total time=   0.2s\n",
            "[CV 3/5] END .....................n_neighbors=4;, score=0.773 total time=   0.2s\n",
            "[CV 4/5] END .....................n_neighbors=4;, score=0.754 total time=   0.2s\n",
            "[CV 5/5] END .....................n_neighbors=4;, score=0.681 total time=   0.2s\n",
            "[CV 1/5] END .....................n_neighbors=5;, score=0.718 total time=   0.2s\n",
            "[CV 2/5] END .....................n_neighbors=5;, score=0.797 total time=   0.2s\n",
            "[CV 3/5] END .....................n_neighbors=5;, score=0.773 total time=   0.2s\n",
            "[CV 4/5] END .....................n_neighbors=5;, score=0.772 total time=   0.2s\n",
            "[CV 5/5] END .....................n_neighbors=5;, score=0.677 total time=   0.2s\n",
            "[CV 1/5] END .....................n_neighbors=6;, score=0.701 total time=   0.2s\n",
            "[CV 2/5] END .....................n_neighbors=6;, score=0.806 total time=   0.1s\n",
            "[CV 3/5] END .....................n_neighbors=6;, score=0.756 total time=   0.1s\n",
            "[CV 4/5] END .....................n_neighbors=6;, score=0.760 total time=   0.1s\n",
            "[CV 5/5] END .....................n_neighbors=6;, score=0.670 total time=   0.2s\n",
            "[CV 1/5] END .....................n_neighbors=7;, score=0.711 total time=   0.2s\n",
            "[CV 2/5] END .....................n_neighbors=7;, score=0.808 total time=   0.2s\n",
            "[CV 3/5] END .....................n_neighbors=7;, score=0.780 total time=   0.2s\n",
            "[CV 4/5] END .....................n_neighbors=7;, score=0.767 total time=   0.2s\n",
            "[CV 5/5] END .....................n_neighbors=7;, score=0.681 total time=   0.2s\n",
            "[CV 1/5] END .....................n_neighbors=8;, score=0.696 total time=   0.2s\n",
            "[CV 2/5] END .....................n_neighbors=8;, score=0.797 total time=   0.2s\n",
            "[CV 3/5] END .....................n_neighbors=8;, score=0.769 total time=   0.2s\n",
            "[CV 4/5] END .....................n_neighbors=8;, score=0.750 total time=   0.1s\n",
            "[CV 5/5] END .....................n_neighbors=8;, score=0.670 total time=   0.2s\n",
            "[CV 1/5] END .....................n_neighbors=9;, score=0.692 total time=   0.2s\n",
            "[CV 2/5] END .....................n_neighbors=9;, score=0.786 total time=   0.2s\n",
            "[CV 3/5] END .....................n_neighbors=9;, score=0.767 total time=   0.1s\n",
            "[CV 4/5] END .....................n_neighbors=9;, score=0.750 total time=   0.2s\n",
            "[CV 5/5] END .....................n_neighbors=9;, score=0.666 total time=   0.2s\n",
            "[CV 1/5] END ....................n_neighbors=10;, score=0.687 total time=   0.2s\n",
            "[CV 2/5] END ....................n_neighbors=10;, score=0.788 total time=   0.2s\n",
            "[CV 3/5] END ....................n_neighbors=10;, score=0.769 total time=   0.2s\n",
            "[CV 4/5] END ....................n_neighbors=10;, score=0.738 total time=   0.2s\n",
            "[CV 5/5] END ....................n_neighbors=10;, score=0.650 total time=   0.2s\n",
            "[CV 1/5] END ....................n_neighbors=11;, score=0.683 total time=   0.2s\n",
            "[CV 2/5] END ....................n_neighbors=11;, score=0.793 total time=   0.2s\n",
            "[CV 3/5] END ....................n_neighbors=11;, score=0.751 total time=   0.1s\n",
            "[CV 4/5] END ....................n_neighbors=11;, score=0.739 total time=   0.2s\n",
            "[CV 5/5] END ....................n_neighbors=11;, score=0.661 total time=   0.2s\n",
            "[CV 1/5] END ....................n_neighbors=12;, score=0.683 total time=   0.2s\n",
            "[CV 2/5] END ....................n_neighbors=12;, score=0.777 total time=   0.2s\n",
            "[CV 3/5] END ....................n_neighbors=12;, score=0.734 total time=   0.2s\n",
            "[CV 4/5] END ....................n_neighbors=12;, score=0.750 total time=   0.2s\n",
            "[CV 5/5] END ....................n_neighbors=12;, score=0.662 total time=   0.2s\n",
            "[CV 1/5] END ....................n_neighbors=13;, score=0.678 total time=   0.2s\n",
            "[CV 2/5] END ....................n_neighbors=13;, score=0.788 total time=   0.2s\n",
            "[CV 3/5] END ....................n_neighbors=13;, score=0.738 total time=   0.2s\n",
            "[CV 4/5] END ....................n_neighbors=13;, score=0.732 total time=   0.2s\n",
            "[CV 5/5] END ....................n_neighbors=13;, score=0.653 total time=   0.1s\n",
            "[CV 1/5] END ....................n_neighbors=14;, score=0.672 total time=   0.2s\n",
            "[CV 2/5] END ....................n_neighbors=14;, score=0.780 total time=   0.1s\n",
            "[CV 3/5] END ....................n_neighbors=14;, score=0.731 total time=   0.2s\n",
            "[CV 4/5] END ....................n_neighbors=14;, score=0.732 total time=   0.2s\n",
            "[CV 5/5] END ....................n_neighbors=14;, score=0.657 total time=   0.2s\n",
            "[CV 1/5] END ....................n_neighbors=15;, score=0.663 total time=   0.2s\n",
            "[CV 2/5] END ....................n_neighbors=15;, score=0.778 total time=   0.2s\n",
            "[CV 3/5] END ....................n_neighbors=15;, score=0.733 total time=   0.2s\n",
            "[CV 4/5] END ....................n_neighbors=15;, score=0.728 total time=   0.2s\n",
            "[CV 5/5] END ....................n_neighbors=15;, score=0.662 total time=   0.2s\n",
            "[CV 1/5] END ....................n_neighbors=16;, score=0.672 total time=   0.2s\n",
            "[CV 2/5] END ....................n_neighbors=16;, score=0.764 total time=   0.1s\n",
            "[CV 3/5] END ....................n_neighbors=16;, score=0.736 total time=   0.2s\n",
            "[CV 4/5] END ....................n_neighbors=16;, score=0.730 total time=   0.2s\n",
            "[CV 5/5] END ....................n_neighbors=16;, score=0.655 total time=   0.2s\n",
            "[CV 1/5] END ....................n_neighbors=17;, score=0.658 total time=   0.2s\n",
            "[CV 2/5] END ....................n_neighbors=17;, score=0.767 total time=   0.2s\n",
            "[CV 3/5] END ....................n_neighbors=17;, score=0.718 total time=   0.2s\n",
            "[CV 4/5] END ....................n_neighbors=17;, score=0.727 total time=   0.2s\n",
            "[CV 5/5] END ....................n_neighbors=17;, score=0.655 total time=   0.2s\n",
            "[CV 1/5] END ....................n_neighbors=18;, score=0.667 total time=   0.2s\n",
            "[CV 2/5] END ....................n_neighbors=18;, score=0.769 total time=   0.1s\n",
            "[CV 3/5] END ....................n_neighbors=18;, score=0.705 total time=   0.2s\n",
            "[CV 4/5] END ....................n_neighbors=18;, score=0.730 total time=   0.2s\n",
            "[CV 5/5] END ....................n_neighbors=18;, score=0.644 total time=   0.2s\n",
            "[CV 1/5] END ....................n_neighbors=19;, score=0.654 total time=   0.1s\n",
            "[CV 2/5] END ....................n_neighbors=19;, score=0.766 total time=   0.2s\n",
            "[CV 3/5] END ....................n_neighbors=19;, score=0.703 total time=   0.1s\n",
            "[CV 4/5] END ....................n_neighbors=19;, score=0.721 total time=   0.2s\n",
            "[CV 5/5] END ....................n_neighbors=19;, score=0.637 total time=   0.1s\n",
            "[CV 1/5] END ....................n_neighbors=20;, score=0.637 total time=   0.2s\n",
            "[CV 2/5] END ....................n_neighbors=20;, score=0.758 total time=   0.1s\n",
            "[CV 3/5] END ....................n_neighbors=20;, score=0.700 total time=   0.2s\n",
            "[CV 4/5] END ....................n_neighbors=20;, score=0.719 total time=   0.2s\n",
            "[CV 5/5] END ....................n_neighbors=20;, score=0.635 total time=   0.2s\n",
            "[CV 1/5] END ....................n_neighbors=21;, score=0.645 total time=   0.1s\n",
            "[CV 2/5] END ....................n_neighbors=21;, score=0.756 total time=   0.2s\n",
            "[CV 3/5] END ....................n_neighbors=21;, score=0.701 total time=   0.2s\n",
            "[CV 4/5] END ....................n_neighbors=21;, score=0.705 total time=   0.2s\n",
            "[CV 5/5] END ....................n_neighbors=21;, score=0.631 total time=   0.2s\n",
            "[CV 1/5] END ....................n_neighbors=22;, score=0.641 total time=   0.2s\n",
            "[CV 2/5] END ....................n_neighbors=22;, score=0.740 total time=   0.2s\n",
            "[CV 3/5] END ....................n_neighbors=22;, score=0.694 total time=   0.2s\n",
            "[CV 4/5] END ....................n_neighbors=22;, score=0.708 total time=   0.2s\n",
            "[CV 5/5] END ....................n_neighbors=22;, score=0.635 total time=   0.2s\n",
            "[CV 1/5] END ....................n_neighbors=23;, score=0.641 total time=   0.2s\n",
            "[CV 2/5] END ....................n_neighbors=23;, score=0.745 total time=   0.2s\n",
            "[CV 3/5] END ....................n_neighbors=23;, score=0.687 total time=   0.2s\n",
            "[CV 4/5] END ....................n_neighbors=23;, score=0.706 total time=   0.2s\n",
            "[CV 5/5] END ....................n_neighbors=23;, score=0.624 total time=   0.2s\n",
            "[CV 1/5] END ....................n_neighbors=24;, score=0.634 total time=   0.2s\n",
            "[CV 2/5] END ....................n_neighbors=24;, score=0.745 total time=   0.2s\n",
            "[CV 3/5] END ....................n_neighbors=24;, score=0.692 total time=   0.2s\n",
            "[CV 4/5] END ....................n_neighbors=24;, score=0.703 total time=   0.2s\n",
            "[CV 5/5] END ....................n_neighbors=24;, score=0.633 total time=   0.2s\n",
            "[CV 1/5] END ....................n_neighbors=25;, score=0.652 total time=   0.2s\n",
            "[CV 2/5] END ....................n_neighbors=25;, score=0.744 total time=   0.2s\n",
            "[CV 3/5] END ....................n_neighbors=25;, score=0.690 total time=   0.1s\n",
            "[CV 4/5] END ....................n_neighbors=25;, score=0.692 total time=   0.2s\n",
            "[CV 5/5] END ....................n_neighbors=25;, score=0.637 total time=   0.1s\n",
            "[CV 1/5] END ....................n_neighbors=26;, score=0.643 total time=   0.2s\n",
            "[CV 2/5] END ....................n_neighbors=26;, score=0.736 total time=   0.2s\n",
            "[CV 3/5] END ....................n_neighbors=26;, score=0.689 total time=   0.2s\n",
            "[CV 4/5] END ....................n_neighbors=26;, score=0.692 total time=   0.2s\n",
            "[CV 5/5] END ....................n_neighbors=26;, score=0.637 total time=   0.2s\n",
            "[CV 1/5] END ....................n_neighbors=27;, score=0.647 total time=   0.2s\n",
            "[CV 2/5] END ....................n_neighbors=27;, score=0.723 total time=   0.2s\n",
            "[CV 3/5] END ....................n_neighbors=27;, score=0.687 total time=   0.1s\n",
            "[CV 4/5] END ....................n_neighbors=27;, score=0.699 total time=   0.2s\n",
            "[CV 5/5] END ....................n_neighbors=27;, score=0.624 total time=   0.2s\n",
            "[CV 1/5] END ....................n_neighbors=28;, score=0.639 total time=   0.2s\n",
            "[CV 2/5] END ....................n_neighbors=28;, score=0.720 total time=   0.2s\n",
            "[CV 3/5] END ....................n_neighbors=28;, score=0.689 total time=   0.2s\n",
            "[CV 4/5] END ....................n_neighbors=28;, score=0.688 total time=   0.1s\n",
            "[CV 5/5] END ....................n_neighbors=28;, score=0.615 total time=   0.2s\n",
            "[CV 1/5] END ....................n_neighbors=29;, score=0.636 total time=   0.2s\n",
            "[CV 2/5] END ....................n_neighbors=29;, score=0.714 total time=   0.2s\n",
            "[CV 3/5] END ....................n_neighbors=29;, score=0.690 total time=   0.2s\n",
            "[CV 4/5] END ....................n_neighbors=29;, score=0.690 total time=   0.2s\n",
            "[CV 5/5] END ....................n_neighbors=29;, score=0.613 total time=   0.2s\n",
            "[CV 1/5] END ....................n_neighbors=30;, score=0.630 total time=   0.2s\n",
            "[CV 2/5] END ....................n_neighbors=30;, score=0.716 total time=   0.1s\n",
            "[CV 3/5] END ....................n_neighbors=30;, score=0.678 total time=   0.2s\n",
            "[CV 4/5] END ....................n_neighbors=30;, score=0.692 total time=   0.2s\n",
            "[CV 5/5] END ....................n_neighbors=30;, score=0.604 total time=   0.2s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['BestKNNProfileWhite.npy']"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l = 'Best Params of KNN with Profile White: '+str(gsKNN.best_params_)+'\\n'\n",
        "l2 = 'Best Accuracy of KNN with Profile White: '+str(gsKNN.best_score_)+'\\n'\n",
        "print(l)\n",
        "print(l2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXzKz2vzZ6-O",
        "outputId": "4b6cb6ba-a593-4b3d-e991-5691dcf4c07d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Params of KNN with Profile White: {'n_neighbors': 1}\n",
            "\n",
            "Best Accuracy of KNN with Profile White: 0.7496219376953321\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#RandomForest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rfc=RandomForestClassifier(random_state=42, probability = True)\n",
        "param_grid = { \n",
        "    'n_estimators': [50, 200],\n",
        "    'max_depth' : [4,5,6,7,8],\n",
        "    'criterion' :['entropy']\n",
        "}\n",
        "  \n",
        "# defining parameter range\n",
        "gsRFC = GridSearchCV(estimator=rfc, \n",
        "                     param_grid=param_grid, \n",
        "                     cv=StratifiedKFold(5).split(X_train, y_train), \n",
        "                    scoring='accuracy',\n",
        "                    verbose=3)\n",
        "gsRFC.fit(X_train, y_train)\n",
        "  \n",
        "# save the model to disk\n",
        "joblib.dump(gsRFC.best_estimator_, 'FinalRFCProfileWhite.npy')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "vvrFdbcRJf8D",
        "outputId": "e4c36470-6401-463e-d229-70be4bfd89de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-0a87f62b0b19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#RandomForest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrfc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobability\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m param_grid = { \n\u001b[1;32m      5\u001b[0m     \u001b[0;34m'n_estimators'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'probability'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l = 'Best Params of RFC with profile White: '+str(gsRFC.best_params_)+'\\n'\n",
        "l2 = 'Best Accuracy of RFC with profile White: '+str(gsRFC.best_score_)+'\\n'\n",
        "print(l)\n",
        "print(l2)"
      ],
      "metadata": {
        "id": "176pJKWHtLJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMy8sd7_lfN-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07b76e67-8e98-4c86-f75a-85f43012ec46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNeighborsClassifier(n_neighbors=1)\n"
          ]
        }
      ],
      "source": [
        "# cv results\n",
        "cv_results = pd.DataFrame(gs.cv_results_)\n",
        "cv_results\n",
        "print(gsKNN.best_estimator_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJ1M-I20mmHp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4df76d51-c044-47ef-b479-23acae255836"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The best test score of SVMis 0.8295312027422119 corresponding to hyperparameters {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "The best test score of KNNis 0.7496219376953321 corresponding to hyperparameters {'n_neighbors': 1}\n",
            "The best test score of KNNis 0.8078986456968108 corresponding to hyperparameters {'criterion': 'entropy', 'max_depth': 8, 'n_estimators': 200}\n"
          ]
        }
      ],
      "source": [
        "# printing the optimal accuracy score and hyperparameters\n",
        "best_score = gs.best_score_\n",
        "best_hyperparams = gs.best_params_\n",
        "\n",
        "print(\"The best test score of SVMis {0} corresponding to hyperparameters {1}\".format(best_score, best_hyperparams))\n",
        "\n",
        "best_scoreKNN = gsKNN.best_score_\n",
        "best_hyperparamsKNN = gsKNN.best_params_\n",
        "\n",
        "print(\"The best test score of KNNis {0} corresponding to hyperparameters {1}\".format(best_scoreKNN, best_hyperparamsKNN))\n",
        "\n",
        "best_scoreRFC = gsRFC.best_score_\n",
        "best_hyperparamsRFC = gsRFC.best_params_\n",
        "\n",
        "print(\"The best test score of KNNis {0} corresponding to hyperparameters {1}\".format(best_scoreRFC, best_hyperparamsRFC))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Af5OaL4m0N6"
      },
      "source": [
        "Building and Evaluating the Final Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVEfNOnvm1HG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "894e1142-82f1-4763-fe4b-62c7d2d778c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVAL accuracy 0.8284457478005866 \n",
            "\n",
            "[[ 4  0  0 ...  0  0  0]\n",
            " [ 0  9  0 ...  0  0  0]\n",
            " [ 0  0 11 ...  0  0  0]\n",
            " ...\n",
            " [ 0  0  0 ...  9  0  0]\n",
            " [ 0  0  0 ...  0  9  0]\n",
            " [ 0  1  1 ...  0  0  6]] \n",
            "\n",
            "EVAL accuracy 0.7859237536656891 \n",
            "\n",
            "[[6 0 0 ... 0 0 0]\n",
            " [0 6 1 ... 0 0 0]\n",
            " [0 0 8 ... 0 0 1]\n",
            " ...\n",
            " [0 0 0 ... 8 0 0]\n",
            " [0 0 0 ... 0 5 0]\n",
            " [0 1 1 ... 0 0 7]] \n",
            "\n",
            "EVAL accuracy 0.8108504398826979 \n",
            "\n",
            "[[2 0 0 ... 0 0 0]\n",
            " [0 9 1 ... 0 0 0]\n",
            " [0 0 8 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 8 0 0]\n",
            " [0 0 0 ... 0 7 0]\n",
            " [0 0 1 ... 0 0 5]] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# model with optimal hyperparameters\n",
        "\n",
        "# model SVC\n",
        "model = joblib.load('BestSVCProfileWhite.npy')\n",
        "\n",
        "y_pred = model.predict(X_eval)\n",
        "\n",
        "# metrics\n",
        "print(\"EVAL accuracy\", metrics.accuracy_score(y_eval, y_pred), \"\\n\")\n",
        "print(metrics.confusion_matrix(y_eval, y_pred), \"\\n\")\n",
        "\n",
        "\n",
        "#MODEL KNN\n",
        "modelKNN = joblib.load('BestKNNProfileWhite.npy')\n",
        "\n",
        "y_pred = modelKNN.predict(X_eval)\n",
        "\n",
        "# metrics\n",
        "print(\"EVAL accuracy\", metrics.accuracy_score(y_eval, y_pred), \"\\n\")\n",
        "print(metrics.confusion_matrix(y_eval, y_pred), \"\\n\")\n",
        "\n",
        "\n",
        "#MODEL RFC\n",
        "modelRFC = joblib.load('BestRFCProfileWhite.npy')\n",
        "\n",
        "y_pred = modelRFC.predict(X_eval)\n",
        "\n",
        "# metrics\n",
        "print(\"EVAL accuracy\", metrics.accuracy_score(y_eval, y_pred), \"\\n\")\n",
        "print(metrics.confusion_matrix(y_eval, y_pred), \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###Parralel Ensemble\n",
        "def find_top_3(predictions_proba):\n",
        "\n",
        "  characters = \"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\n",
        "  predictions=[]\n",
        "\n",
        "  predictions_proba = [round(member, 2) for member in predictions_proba]\n",
        "\n",
        "  max_number = max(predictions_proba)\n",
        "  index1 = predictions_proba.index(max_number)\n",
        "  predictions.append(characters[index1])\n",
        "  predictions.append(predictions_proba[index1]) \n",
        "  predictions_proba[index1] = 0\n",
        "\n",
        "  index2 = predictions_proba.index(max(predictions_proba))\n",
        "  predictions.append(characters[index2])\n",
        "  predictions.append(predictions_proba[index2]) \n",
        "  predictions_proba[index2] = 0\n",
        "\n",
        "  index3 = predictions_proba.index(max(predictions_proba))\n",
        "  predictions.append(characters[index3])\n",
        "  predictions.append(predictions_proba[index3]) \n",
        "  predictions_proba[index3] = 0\n",
        " \n",
        "  return predictions_proba, predictions\n",
        "\n",
        "def Parallel_Ensemb(input, model1, model2, model3, w1, w2, w3):\n",
        "\n",
        "  y_model1_proba = model1.predict_proba(input)\n",
        "  y_model1_proba = y_model1_proba.tolist()\n",
        "  y_model1_proba = [value * w1 for value in y_model1_proba]\n",
        "\n",
        "  y_model2_proba = model2.predict_proba(input)\n",
        "  y_model2_proba = y_model2_proba.tolist()\n",
        "  y_model2_proba = [value * w2 for value in y_model2_proba]\n",
        "\n",
        "  y_model3_proba = model3.predict_proba(input)\n",
        "  y_model3_proba = y_model3_proba.tolist()\n",
        "  y_model3_proba = [value * w3 for value in y_model3_proba]\n",
        "\n",
        "  final_y_proba = sum = [x + y for (x, y) in zip(y_model1_proba, y_model2_proba)] \n",
        "  final_y_proba = sum = [x + y for (x, y) in zip(final_y_proba, y_model3_proba)] \n",
        "  \n",
        "  _, predictions = find_top_3(final_y_proba)\n",
        "\n",
        "  return predictions\n"
      ],
      "metadata": {
        "id": "UARKRacjWuff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "model1 = joblib.load('FinalSVCProfileWhite.npy')\n",
        "model2 = joblib.load('BestKNNHISTWhite.npy')\n",
        "model3 = joblib.load('BestRFCProfileWhite.npy')\n",
        "\n",
        "y_pred1 = model1.predict_proba(X_eval).tolist()\n",
        "y_pred2 = model2.predict_proba(X_eval).tolist()\n",
        "y_pred3 = model3.predict_proba(X_eval).tolist()\n",
        "print(len(y_pred1[57]))\n",
        "print(len(y_pred2[57]))\n",
        "print(len(y_pred3[57]))\n",
        "\n",
        "# y_pred1 = [\"%.2f\" % member for member in y_pred1[0]]\n",
        "# y_pred2 = [\"%.2f\" % member for member in y_pred2[0]]\n",
        "# y_pred3 = [\"%.2f\" % member for member in y_pred3[0]]\n",
        "# print(y_pred1)\n",
        "y_pred_proba1 = [value * 0.3 for value in y_pred1[57]]\n",
        "y_pred_proba2 = [value * 0.4 for value in y_pred[57]]\n",
        "y_pred_proba3 = [value * 0.3 for value in y_pred3[57]]\n",
        "\n",
        "y_pred_proba = sum = [x + y for (x, y) in zip(y_pred_proba1, y_pred_proba2)] \n",
        "y_pred_proba = sum = [x + y for (x, y) in zip(y_pred_proba, y_pred_proba3)] \n",
        "print(len(y_pred_proba))\n",
        "# print(y_pred_proba)\n",
        "_, predictions = find_top_3(y_pred_proba)\n",
        "print(predictions)\n",
        "# y_pred3 = [\"%.2f\" % member for member in y_pred3[10]]\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6u3FSCoWvSn",
        "outputId": "23d36b0a-e610-404a-cf6f-f3850667396d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "62\n",
            "62\n",
            "62\n",
            "62\n",
            "[0.01, 0.0, 0.0, 0.02, 0.0, 0.66, 0.02, 0.0, 0.01, 0.01, 0.0, 0.01, 0.0, 0.0, 0.04, 0.01, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "0.66\n",
            "5\n",
            "5\n",
            "['5', 0.66, 'S', 0.06, 'E', 0.04]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}